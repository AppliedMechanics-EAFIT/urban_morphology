import geopandas as gpd
import os
import osmnx as ox
import json
import matplotlib.pyplot as plt
import networkx as nx
import pandas as pd
import re
import shapely
from shapely.geometry import Polygon, MultiPolygon
import plotly.graph_objects as go
import numpy as np
import matplotlib.patches as mpatches
import sys
import ast
import seaborn as sns
from scipy.stats import f_oneway, kruskal
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.patches as mpatches
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, confusion_matrix, classification_report , calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from matplotlib import colors as mcolors
from shapely.wkt import loads
from shapely.geometry import shape
import scipy.stats as stats
from matplotlib.colors import ListedColormap
from networkx.algorithms.community import modularity
from matplotlib.patches import Patch
from math import degrees, atan2
import fiona
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)





# # Ruta a la carpeta .gdb (¬°no un archivo espec√≠fico!)
# gdb_path = "Pasto/AJUSTE_POT_PASTO_2023.gdb"

# # Crear carpeta de salida para geojson (opcional)
# output_dir = "GeoJSON_Export"
# os.makedirs(output_dir, exist_ok=True)

# # 1. Listar capas disponibles
# print("\nüìÅ Buscando capas en la GDB...")
# layers = fiona.listlayers(gdb_path)
# print(f"‚úîÔ∏è Capas encontradas ({len(layers)}):")
# for i, layer in enumerate(layers):
#     print(f"{i+1}. {layer}")

# # 2. Leer, mostrar y visualizar cada capa
# for layer in layers:
#     print(f"\nüîç Leyendo capa: {layer}")
#     gdf = gpd.read_file(gdb_path, layer=layer)

#     # Mostrar propiedades
#     print("üìå Columnas:", gdf.columns.tolist())
#     print("üìê Total de geometr√≠as:", len(gdf))
#     print("üåê Sistema de coordenadas (CRS):", gdf.crs)
#     print(gdf.head())

#     # 3. Visualizar
#     gdf.plot(figsize=(8, 6), edgecolor="black", cmap="Set2")
#     plt.title(f"Capa: {layer}")
#     plt.xlabel("Longitud")
#     plt.ylabel("Latitud")
#     plt.tight_layout()
#     plt.show()

#     # 4. Guardar como GeoJSON
#     output_path = os.path.join(output_dir, f"{layer}.geojson")
#     gdf.to_file(output_path, driver="GeoJSON")
#     print(f"üíæ Guardado como GeoJSON: {output_path}")



def convert_shapefile_to_geojson(shapefile_paths, output_directory):
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)

    geojson_data = {}

    for shapefile_path in shapefile_paths:
        gdf = gpd.read_file(shapefile_path)

        # Convert to EPSG:4326 if needed
        if gdf.crs != "EPSG:4326":
            gdf = gdf.to_crs(epsg=4326)

        # Convert the geodataframe to GeoJSON format
        geojson_string = gdf.to_json()

        # Define the output JSON file name
        shapefile_name = os.path.basename(shapefile_path).replace(".shp", ".geojson")
        output_path = os.path.join(output_directory, shapefile_name)

        # Save the GeoJSON file
        with open(output_path, "w") as f:
            f.write(geojson_string)

        # Store the file path in the result dictionary
        geojson_data[shapefile_path] = output_path

    return geojson_data







def plot_road_network_from_geojson(geojson_path, network_type, simplify=True):
    """
    Loads a GeoJSON file, extracts the polygon boundary, retrieves the road network,
    and plots the network with distinct colors for polygons, road links, and nodes.

    Parameters:
    - geojson_path (str): Path to the GeoJSON file.
    - network_type (str): Type of roads to retrieve ('all', 'drive', 'walk', etc.).
    - simplify (bool): Whether to simplify the graph for better visualization.

    Returns:
    - None (plots the figure).
    """

    # Load the GeoJSON file
    gdf = gpd.read_file(geojson_path)

    # Get the exact polygon boundary (avoid convex hull if possible)
    polygon = gdf.unary_union

    # Retrieve the road network from OSMnx
    G = ox.graph_from_polygon(polygon, network_type=network_type, simplify=simplify)

    # Convert graph to GeoDataFrame for better node handling
    nodes, edges = ox.graph_to_gdfs(G)

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 12))

    # Plot the boundary polygons with distinct colors
    gdf.plot(ax=ax, facecolor="lightgray", edgecolor="blue", linewidth=2, alpha=0.5, linestyle="--", label="Boundary Polygons")

    # Plot the road network edges in black
    edges.plot(ax=ax, edgecolor="black", linewidth=0.3, alpha=0.6, label="Road Links")

    # Plot the road network (links) in black
    ox.plot_graph(G, ax=ax, node_size=0, edge_linewidth=0.1, edge_color="black")

def get_street_network_metrics_per_polygon(
    geojson_path,
    network_type='drive',
    output_txt='stats_output.txt'
    ):
    """
    Lee un archivo GeoJSON que contiene uno o varios pol√≠gonos (o multipol√≠gonos)
    y, para cada pol√≠gono/sub-pol√≠gono, calcula estad√≠sticas de la red vial usando OSMnx.
    Luego, almacena los resultados en un archivo de texto, con un √≠ndice que
    identifica cada pol√≠gono procesado.

    Par√°metros:
    -----------
    geojson_path : str
        Ruta al archivo GeoJSON.
    network_type : str
        Tipo de v√≠as a recuperar ('all', 'drive', 'walk', etc.). Por defecto 'drive'.
    output_txt : str
        Nombre o ruta del archivo .txt donde se guardar√°n los resultados.

    Retorna:
    --------
    None. Escribe un archivo .txt con las estad√≠sticas de cada pol√≠gono.
    """

    # ---------------------------------------------------------------------
    # 0. Si ya existe el .txt, leer su contenido previo
    #    y detectar qu√© Pol√≠gono/SubPol√≠gono se han calculado.
    # ---------------------------------------------------------------------
    old_lines = []
    processed_pairs = set()  # para almacenar (idx, sub_idx)

    if os.path.exists(output_txt):
        with open(output_txt, 'r', encoding='utf-8') as old_file:
            old_lines = old_file.readlines()

        # Buscar l√≠neas con el patr√≥n: "=== Pol√≠gono X - SubPol√≠gono Y ==="
        pattern = r"=== Pol√≠gono (\d+) - SubPol√≠gono (\d+) ==="
        for line in old_lines:
            match = re.search(pattern, line)
            if match:
                i_str, s_str = match.groups()
                processed_pairs.add((int(i_str), int(s_str)))

    # ---------------------------------------------------------------------
    # 1. Cargar el GeoJSON como un GeoDataFrame
    # ---------------------------------------------------------------------
    gdf = gpd.read_file(geojson_path)

    # ---------------------------------------------------------------------
    # 2. Abrimos el archivo de salida (modo 'w') para sobrescribir
    #    pero primero volvemos a escribir lo que hab√≠a antes
    # ---------------------------------------------------------------------
    os.makedirs(os.path.dirname(output_txt) or '.', exist_ok=True)
    with open(output_txt, 'w', encoding='utf-8') as f:

        # Reescribir el contenido previo (si exist√≠a)
        for line in old_lines:
            f.write(line)

        # -----------------------------------------------------------------
        # 3. Iterar sobre cada fila (cada 'feature') del GeoDataFrame
        # -----------------------------------------------------------------
        for idx, row in gdf.iterrows():
            geometry = row.geometry

            if geometry is None or geometry.is_empty:
                # Si la geometr√≠a est√° vac√≠a, la ignoramos
                f.write(f"\n=== Pol√≠gono {idx}: GEOMETR√çA VAC√çA ===\n")
                continue

            # Determinar si es Polygon o MultiPolygon
            if geometry.geom_type == 'Polygon':
                polygons_list = [geometry]
            elif geometry.geom_type == 'MultiPolygon':
                polygons_list = list(geometry.geoms)
            else:
                # Si es otro tipo de geometr√≠a (LineString, Point, etc.), saltar
                f.write(f"\n=== Pol√≠gono {idx}: Tipo de geometr√≠a no v√°lido ({geometry.geom_type}) ===\n")
                continue

            # -----------------------------------------------------------------
            # 4. Procesar cada sub-pol√≠gono
            # -----------------------------------------------------------------
            for sub_idx, poly in enumerate(polygons_list):

                # Si ya est√° en processed_pairs, no recalculamos
                if (idx, sub_idx) in processed_pairs:
                    print(f"Saltando Pol√≠gono {idx} - SubPol√≠gono {sub_idx}: ya existe en {output_txt}")
                    continue

                try:
                    G = ox.graph_from_polygon(
                        poly,
                        network_type=network_type,
                        simplify=True
                    )
                except Exception as e:
                    f.write(f"\n--- Pol√≠gono {idx}-{sub_idx}: ERROR al crear la red ---\n{e}\n")
                    continue

                # Verificar si el grafo tiene aristas
                if len(G.edges()) == 0:
                    f.write(f"\n--- Pol√≠gono {idx}-{sub_idx}: Grafo vac√≠o (sin v√≠as) ---\n")
                    continue

                # Calcular estad√≠sticas
                stats = ox.stats.basic_stats(G)

                # Calcular √°rea de este sub-pol√≠gono en km¬≤
                area_km2 = (
                    gpd.GeoDataFrame(geometry=[poly], crs=gdf.crs)
                    .to_crs(epsg=3395)
                    .geometry.area.sum() / 1e6
                )

                intersection_count = stats.get("intersection_count", 0)
                street_length_total = stats.get("street_length_total", 0.0)

                if area_km2 > 0:
                    intersection_density_km2 = intersection_count / area_km2
                    street_density_km2 = street_length_total / area_km2
                else:
                    intersection_density_km2 = 0
                    street_density_km2 = 0

                stats["intersection_density_km2"] = intersection_density_km2
                stats["street_density_km2"] = street_density_km2
                stats["area_km2"] = area_km2

                # -----------------------------------------------------------------
                # 4.5 Guardar resultados en el archivo de texto
                # -----------------------------------------------------------------
                f.write(f"\n=== Pol√≠gono {idx} - SubPol√≠gono {sub_idx} ===\n")
                for k, v in stats.items():
                    f.write(f"{k}: {v}\n")

    print(f"Resultados guardados en: {output_txt}")

def ordenar_y_limpiar_txt(input_txt, output_txt):
    """
    Lee un archivo .txt con bloques relacionados a "Pol√≠gono X - SubPol√≠gono Y",
    o l√≠neas de red vac√≠a y errores, los ordena, elimina duplicados
    y genera un nuevo archivo .txt.

    Par√°metros:
    -----------
    input_txt : str
        Ruta al archivo original desordenado.
    output_txt : str
        Ruta al archivo de salida ya ordenado y sin duplicados.

    Ejemplo de bloques reconocidos:
      "=== Pol√≠gono 181 - SubPol√≠gono 0 ==="
      "--- Pol√≠gono 24-0: Grafo vac√≠o (sin v√≠as) ---"
      "=== Pol√≠gono 12: GEOMETR√çA VAC√çA ==="
    """

    # 1. Leer todas las l√≠neas
    if not os.path.exists(input_txt):
        print(f"Archivo {input_txt} no existe.")
        return

    with open(input_txt, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # 2. Expresiones regulares para reconocer "cabeceras"
    #    Bloques que inician con "=== Pol√≠gono X - SubPol√≠gono Y ==="
    #    o "=== Pol√≠gono X: ..." (geom vac√≠a) o "--- Pol√≠gono X-Y: Grafo vac√≠o..."
    # ------------------------------------------------------------------
    #    Vamos a capturar:
    #      pol_idx -> \d+     (grupo 1)
    #      sub_idx -> \d+     (grupo 2), opcional
    #      type/block -> para distinguir "estad√≠sticas", "vac√≠o", "error", etc.
    #    
    #    Para simplificar, usaremos varios patrones y vemos cu√°l coincide.
    # ------------------------------------------------------------------

    # Caso A: "=== Pol√≠gono 181 - SubPol√≠gono 0 ==="
    pattern_stats = re.compile(r'^=== Pol√≠gono\s+(\d+)\s*-\s*SubPol√≠gono\s+(\d+)\s*===\s*$')
    # Caso B: "--- Pol√≠gono 24-0: Grafo vac√≠o (sin v√≠as) ---"
    pattern_empty = re.compile(r'^--- Pol√≠gono\s+(\d+)-(\d+):\s*(.+)---$')
    # Caso C: "=== Pol√≠gono 181: GEOMETR√çA VAC√çA ==="
    pattern_geom = re.compile(r'^=== Pol√≠gono\s+(\d+):\s*(.+)===\s*$')

    # Vamos a recorrer las l√≠neas y construir "bloques" de texto.
    # Cada vez que detectamos una cabecera, creamos una nueva entrada.
    data_dict = {}  
    # clave -> (pol_idx, sub_idx, tipo) 
    # valor -> list of lines (contenido)

    current_key = None
    current_block = []

    def save_block(key, block):
        """Guarda el bloque en data_dict, sobrescribiendo si la clave ya exist√≠a."""
        if key is None or not block:
            return
        # Sobrescribimos => la "√∫ltima" versi√≥n de un pol√≠gono es la que prevalece
        data_dict[key] = block

    for line in lines:
        line_stripped = line.strip('\n')

        # ¬øCoincide con pattern_stats?
        match_stats = pattern_stats.match(line_stripped)
        if match_stats:
            # Antes de iniciar nuevo bloque, guardamos el anterior
            save_block(current_key, current_block)
            current_block = [line]  # empezamos un nuevo bloque con esta l√≠nea
            pol = int(match_stats.group(1))
            sub = int(match_stats.group(2))
            current_key = (pol, sub, "stats") 
            continue

        # ¬øCoincide con pattern_empty? (ej: Grafo vac√≠o)
        match_empty = pattern_empty.match(line_stripped)
        if match_empty:
            save_block(current_key, current_block)
            current_block = [line]
            pol = int(match_empty.group(1))
            sub = int(match_empty.group(2))
            info = match_empty.group(3).strip()  # "Grafo vac√≠o (sin v√≠as)"
            current_key = (pol, sub, "empty:" + info)
            continue

        # ¬øCoincide con pattern_geom? (ej: GEOMETR√çA VAC√çA)
        match_geom = pattern_geom.match(line_stripped)
        if match_geom:
            save_block(current_key, current_block)
            current_block = [line]
            pol = int(match_geom.group(1))
            info = match_geom.group(2).strip()  # "GEOMETR√çA VAC√çA" u otro
            # sub_idx = None => usaremos -1 para indicar "sin subpol√≠gono"
            current_key = (pol, -1, "geom:" + info)
            continue

        # Si no coincide, es una l√≠nea dentro del bloque actual
        if current_key is not None:
            current_block.append(line)
        else:
            # No estamos dentro de un bloque (puede ser texto suelto),
            # lo ignoramos o guardamos si quieres.
            pass

    # Al final, guardar el √∫ltimo bloque (si existiera)
    save_block(current_key, current_block)

    # 3. Ahora data_dict tiene las entradas clave -> [lines_de_bloque].
    #    Queremos ordenarlas por pol√≠gono, subpol√≠gono.
    #    Nota: sub_idx podr√≠a ser -1 en caso de "GEOMETR√çA VAC√çA" (sin subpol√≠gono).
    sorted_keys = sorted(data_dict.keys(), key=lambda x: (x[0], x[1]))

    # 4. Escribir el resultado ordenado y sin duplicados en output_txt
    with open(output_txt, 'w', encoding='utf-8') as out:
        for key in sorted_keys:
            block_lines = data_dict[key]
            for bl in block_lines:
                out.write(bl)
        out.write("\n")  # Salto de l√≠nea final

    print(f"Archivo ordenado y limpio guardado en: {output_txt}")

# if __name__ == "__main__":
#     input_file = "Poligonos_Medellin/Resultados/poligonos_stats.txt"   # El .txt ca√≥tico
#     output_file = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
#     ordenar_y_limpiar_txt(input_file, output_file)

def load_polygon_stats_from_txt(stats_txt):
    """
    Lee un archivo de estad√≠sticas (.txt) y devuelve un diccionario
    { (pol_idx, sub_idx): {"k_avg": val, "m": val, ...}, ... }.
    Ignora los casos de 'Grafo vac√≠o' o 'GEOMETR√çA VAC√çA'.
    """

    with open(stats_txt, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    pattern_header = re.compile(r'^=== Pol√≠gono\s+(\d+)\s*-\s*SubPol√≠gono\s+(\d+)\s*===\s*$')
    pattern_kv = re.compile(r'^([^:]+):\s+(.*)$')  # clave: valor

    stats_dict = {}
    current_key = None

    for line in lines:
        line_stripped = line.strip()
        # Detectar encabezado
        match_header = pattern_header.match(line_stripped)
        if match_header:
            pol_idx = int(match_header.group(1))
            sub_idx = int(match_header.group(2))
            current_key = (pol_idx, sub_idx)
            stats_dict[current_key] = {}
            continue

        # Si estamos dentro de un bloque, buscar clave: valor
        if current_key is not None:
            match_kv = pattern_kv.match(line_stripped)
            if match_kv:
                k = match_kv.group(1)
                v = match_kv.group(2)
                # Intentar convertir a float o int
                try:
                    v_num = float(v)
                    # Si es entero sin decimales, lo pasamos a int
                    if v_num.is_integer():
                        v_num = int(v_num)
                    stats_dict[current_key][k] = v_num
                except ValueError:
                    # no se pudo convertir -> guardar como string
                    stats_dict[current_key][k] = v

    return stats_dict

def classify_polygon(poly_stats):
    """
    Clasifica un pol√≠gono (o sub-pol√≠gono) en:
      'cul_de_sac', 'gridiron', 'organico' o 'hibrido'
    usando reglas de decisi√≥n basadas en diversas estad√≠sticas.

    Par√°metros:
    -----------
    poly_stats : dict con claves:
      - "streets_per_node_avg" (float)
      - "streets_per_node_counts" (str o dict)
      - "streets_per_node_proportions" (str o dict)
      - "intersection_density_km2" (float)
      - "circuity_avg" (float)
      - "k_avg" (float)
      - "street_density_km2" (float)
      - etc.

    Retorna:
    --------
    str : 'cul_de_sac', 'gridiron', 'organico' o 'hibrido'
    """

    # -------------------------------------------------------------------
    # 1. Parsear fields que podr√≠an venir como string en lugar de dict
    # -------------------------------------------------------------------
    spn_counts_str = poly_stats.get("streets_per_node_counts", "{}")
    spn_props_str = poly_stats.get("streets_per_node_proportions", "{}")

    if isinstance(spn_counts_str, str):
        try:
            spn_counts = ast.literal_eval(spn_counts_str)
        except:
            spn_counts = {}
    else:
        spn_counts = spn_counts_str

    if isinstance(spn_props_str, str):
        try:
            spn_props = ast.literal_eval(spn_props_str)
        except:
            spn_props = {}
    else:
        spn_props = spn_props_str

    # -------------------------------------------------------------------
    # 2. Extraer Variables Num√©ricas Principales
    # -------------------------------------------------------------------
    streets_per_node = float(poly_stats.get("streets_per_node_avg", 0.0))
    intersection_density = float(poly_stats.get("intersection_density_km2", 0.0))
    circuity = float(poly_stats.get("circuity_avg", 1.0))
    k_avg = float(poly_stats.get("k_avg", 0.0))
    street_density = float(poly_stats.get("street_density_km2", 0.0))

    # Ejemplo de proporciones de nodos grado 1 y 4
    prop_deg1 = float(spn_props.get(1, 0.0))  # nodos con 1 calle
    prop_deg4 = float(spn_props.get(4, 0.0))  # nodos con 4 calles

    # -------------------------------------------------------------------
    # 3. √Årbol de Decisi√≥n (umbrales orientativos)
    # -------------------------------------------------------------------
    #
    # A. Detectar cul-de-sac
    #    Reglas ejemplo:
    #    - Proporci√≥n de nodos grado 1 alta (>= 0.40)
    #    - O (streets_per_node < 2.1 y intersection_density < 30)
    #
    if prop_deg1 >= 0.40 or (streets_per_node < 2.1 and intersection_density < 30):
        return "cul_de_sac"

    # B. Detectar gridiron
    #    - Poca sinuosidad: circuity < 1.03
    #    - Buena conectividad local: streets_per_node >= 3.0
    #    - intersection_density >= 50 (bastante intersecciones)
    #    - prop_deg4 > 0.30 √≥ 0.40 => muchos nodos con 4 salidas
    #
    if (circuity < 1.03) and (streets_per_node >= 3.0 or intersection_density >= 50 or prop_deg4 >= 0.30):
        return "gridiron"

    # C. Detectar org√°nico
    #    - Calles sinuosas => circuity > 1.05
    #    - k_avg > 3.0 => muchos nodos con 3+ aristas
    #    - street_density > 3000 => malla densa
    #
    if (circuity > 1.05) and (k_avg > 3.0) and (street_density > 3000):
        return "organico"

    # D. Caso general => h√≠brido
    return "hibrido"


# def classify_polygon(poly_stats):
#     """
#     Clasifica un pol√≠gono (o sub-pol√≠gono) en:
#       'cul_de_sac', 'gridiron', 'organico' o 'hibrido'
#     basado en la teor√≠a de patrones urbanos y m√©tricas morfol√≥gicas.

#     Par√°metros:
#     -----------
#     poly_stats : dict con claves:
#       - "streets_per_node_avg" (float): Promedio de calles por nodo
#       - "streets_per_node_counts" (str o dict): Conteo de nodos por n√∫mero de calles
#       - "streets_per_node_proportions" (str o dict): Proporci√≥n de nodos por n√∫mero de calles
#       - "intersection_density_km2" (float): Densidad de intersecciones por km¬≤
#       - "circuity_avg" (float): Sinuosidad promedio de segmentos
#       - "k_avg" (float): Grado promedio de nodos
#       - "street_density_km2" (float): Densidad de calles por km¬≤
#       - "orientation_entropy" (float, opcional): Entrop√≠a de orientaci√≥n de segmentos
#       - "edge_length_avg" (float, opcional): Longitud promedio de aristas
#       - "street_length_avg" (float, opcional): Longitud promedio de calles

#     Retorna:
#     --------
#     str : 'cul_de_sac', 'gridiron', 'organico' o 'hibrido'
#     """
    
#     # -------------------------------------------------------------------
#     # 1. Parsear fields que podr√≠an venir como string en lugar de dict
#     # -------------------------------------------------------------------
#     spn_counts_str = poly_stats.get("streets_per_node_counts", "{}")
#     spn_props_str = poly_stats.get("streets_per_node_proportions", "{}")

#     if isinstance(spn_counts_str, str):
#         try:
#             spn_counts = ast.literal_eval(spn_counts_str)
#         except:
#             spn_counts = {}
#     else:
#         spn_counts = spn_counts_str

#     if isinstance(spn_props_str, str):
#         try:
#             spn_props = ast.literal_eval(spn_props_str)
#         except:
#             spn_props = {}
#     else:
#         spn_props = spn_props_str

#     # -------------------------------------------------------------------
#     # 2. Extraer Variables Num√©ricas Principales
#     # -------------------------------------------------------------------
#     # Conectividad y estructura nodal
#     streets_per_node = float(poly_stats.get("streets_per_node_avg", 0.0))
#     k_avg = float(poly_stats.get("k_avg", 0.0))
    
#     # Densidades
#     intersection_density = float(poly_stats.get("intersection_density_km2", 0.0))
#     street_density = float(poly_stats.get("street_density_km2", 0.0))
    
#     # Geometr√≠a
#     circuity = float(poly_stats.get("circuity_avg", 1.0))
#     edge_length_avg = float(poly_stats.get("edge_length_avg", 0.0))
#     street_length_avg = float(poly_stats.get("street_length_avg", 0.0))
    
#     # Entrop√≠a de orientaci√≥n (0=alineado, 1=diverso)
#     orientation_entropy = float(poly_stats.get("orientation_entropy", 0.5))
    
#     # Proporciones de nodos por grado
#     prop_deg1 = float(spn_props.get('1', 0.0))  # callejones sin salida
#     prop_deg3 = float(spn_props.get('3', 0.0))  # intersecciones en T
#     prop_deg4 = float(spn_props.get('4', 0.0))  # intersecciones en cruz

#     # -------------------------------------------------------------------
#     # 3. Clasificaci√≥n por Patrones Urbanos
#     # -------------------------------------------------------------------
    
#     # A. Patr√≥n Cul-de-sac / Suburban
#     # Caracter√≠sticas: Alta proporci√≥n de callejones sin salida, baja conectividad,
#     # estructura jer√°rquica y arborescente, baja densidad de intersecciones
#     if (prop_deg1 >= 0.35 or 
#         (streets_per_node < 2.4 and intersection_density < 40) or
#         (prop_deg1 >= 0.25 and circuity > 1.1 and streets_per_node < 2.5)):
#         return "cul_de_sac"

#     # B. Patr√≥n Gridiron / Reticular
#     # Caracter√≠sticas: Baja sinuosidad, alta proporci√≥n de cruces (nodos grado 4),
#     # buena conectividad, orientaci√≥n consistente (baja entrop√≠a de orientaci√≥n)
#     if ((circuity < 1.05 and prop_deg4 >= 0.25) or
#         (streets_per_node >= 2.8 and orientation_entropy < 0.6 and prop_deg4 > prop_deg3) or
#         (intersection_density >= 70 and circuity < 1.08 and prop_deg1 < 0.15)):
#         return "gridiron"

#     # C. Patr√≥n Org√°nico / Irregular
#     # Caracter√≠sticas: Alta sinuosidad, predominio de intersecciones en T,
#     # alta entrop√≠a de orientaci√≥n, irregularidad geom√©trica
#     if ((circuity > 1.08 and orientation_entropy > 0.7) or
#         (prop_deg3 > prop_deg4 * 1.5 and circuity > 1.05) or
#         (orientation_entropy > 0.8 and street_density > 15000 and circuity > 1.05)):
#         return "organico"

#     # D. Patr√≥n H√≠brido (mezcla de tipos o casos especiales)
#     # Incluyendo subdivisi√≥n si se detectan caracter√≠sticas espec√≠ficas
#     if (streets_per_node > 2.7 and intersection_density > 50 and 
#         0.15 < prop_deg1 < 0.25 and 0.2 < prop_deg4 < 0.3):
#         # H√≠brido con tendencia a ret√≠cula
#         return "hibrido"
    
#     # Caso general - H√≠brido no espec√≠fico
#     return "hibrido"

def add_classification_to_gdf(geojson_path, stats_dict):
    """
    Carga el geojson como gdf, crea una columna 'class' con
    la categor√≠a del pol√≠gono, o None si no hay datos en stats_dict.
    """
    gdf = gpd.read_file(geojson_path).copy()

    # Crear nueva columna
    gdf["class"] = None

    for idx in gdf.index:
        # Si usas "Pol√≠gono idx - SubPol√≠gono 0" => la clave es (idx, 0)
        key = (idx, 0)
        if key in stats_dict:
            # tenemos datos
            poly_stats = stats_dict[key]
            cat = classify_polygon(poly_stats)
            gdf.at[idx, "class"] = cat
        else:
            # No hay datos => se queda como None
            pass

    return gdf

def plot_polygons_classification_png(
    geojson_path,
    stats_dict,
    classify_func,
    output_png="polygons_classification.png"
):
    """
    Lee un GeoDataFrame (geojson_path), asigna una 'clase' a cada pol√≠gono
    seg√∫n las estad√≠sticas en 'stats_dict' y la 'classify_func',
    y dibuja en un PNG (Matplotlib) con colores distintos por clase.

    Se asume que, en 'stats_dict', las claves son (idx, sub_idx).
    Aqu√≠, tomamos solamente sub_idx=0, por ejemplo, si cada fila
    corresponde a (idx, 0). Ajusta si es distinto.
    """

    gdf = gpd.read_file(geojson_path)

    # Crear columna 'pattern' con la clase
    patterns = []
    for idx, row in gdf.iterrows():
        key = (idx, 0)  # si cada fila = sub_poligono 0
        if key in stats_dict:
            poly_stats = stats_dict[key]
            category = classify_func(poly_stats)
        else:
            category = "desconocido"
        patterns.append(category)

    gdf["pattern"] = patterns

    # Mapear cada clase a un color
    color_map = {
        'cul_de_sac': '#FF6B6B',   # Rojo suave
        'gridiron': '#4ECDC4',     # Verde azulado
        'organico': '#45B7D1',     # Azul claro
        'hibrido': '#FDCB6E', 
        "desconocido": "gray"
    }

    def get_color(cat):
        return color_map.get(cat, "black")

    plot_colors = [get_color(cat) for cat in gdf["pattern"]]

    # Graficar
    fig, ax = plt.subplots(figsize=(10, 8))
    gdf.plot(
        ax=ax,
        color=plot_colors,
        edgecolor="black",
        linewidth=0.5
    )

    # Leyenda manual
    legend_patches = []
    for cat, col in color_map.items():
        patch = mpatches.Patch(color=col, label=cat)
        legend_patches.append(patch)
    ax.legend(handles=legend_patches, title="Tipo de pol√≠gono")

    ax.set_title("Clasificaci√≥n de Pol√≠gonos", fontsize=14)
    ax.set_axis_off()

    plt.savefig(output_png, dpi=300, bbox_inches="tight")
    plt.close(fig)
    print(f"Imagen guardada en: {output_png}")

# # =============================================================================
# # Ejemplo de uso
# if __name__ == "__main__":
#     geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA.geojson"
#     stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"

#     # 1. Cargar stats desde .txt
#     stats_dict = load_polygon_stats_from_txt(stats_txt)

#     # 2. Generar PNG con cada fila = (idx, 0)
#     plot_polygons_classification_png(
#         geojson_path=geojson_file,
#         stats_dict=stats_dict,
#         classify_func=classify_polygon,
#         output_png="polygon_classification.png"
#     )

def normalize_edge(x0, y0, x1, y1, tol=4):
    """
    Redondea las coordenadas a 'tol' decimales y retorna una tupla
    ordenada con los dos puntos. De esta forma, el segmento (A,B) es igual a (B,A).
    """
    p1 = (round(x0, tol), round(y0, tol))
    p2 = (round(x1, tol), round(y1, tol))
    return tuple(sorted([p1, p2]))

def plot_street_patterns_classification(
    geojson_path,
    classify_func,
    stats_dict,
    place_name="MyPlace",
    network_type="drive",
    output_folder="Graphs_Cities",
    simplify=True
):
    """
    Genera un HTML interactivo en Plotly con DOS capas:
      1) Capa base vectorial (en gris) de la red completa a partir de la uni√≥n de
         todos los pol√≠gonos, filtrando aquellos segmentos que se sobreponen con los
         de la clasificaci√≥n.
      2) Capas vectoriales de sub-pol√≠gonos coloreados seg√∫n su clasificaci√≥n.
    
    De esta forma, los segmentos repetidos se muestran s√≥lo en la capa clasificada.
    """
    print("Construyendo la capa base (red completa) a partir de la uni√≥n de pol√≠gonos...")
    gdf = gpd.read_file(geojson_path)
    try:
        poly_union = gdf.union_all()  # Si tu geopandas es reciente
    except AttributeError:
        poly_union = gdf.unary_union

    try:
        G_full = ox.graph_from_polygon(poly_union, network_type=network_type, simplify=simplify)
    except Exception as e:
        print(f"Error al crear la red base: {e}")
        return

    if len(G_full.edges()) == 0:
        print("La red base (uni√≥n) est√° vac√≠a. Revisa tu GeoJSON y tipo de red.")
        return

    G_full = ox.project_graph(G_full)
    base_nodes = list(G_full.nodes())
    base_positions = np.array([(G_full.nodes[n]['y'], G_full.nodes[n]['x']) for n in base_nodes])
    x_vals = base_positions[:, 1]
    y_vals = base_positions[:, 0]
    global_x_min, global_x_max = x_vals.min(), x_vals.max()
    global_y_min, global_y_max = y_vals.min(), y_vals.max()

    # --------------------------------------------------------------------------------
    # B) CAPAS DE SUB-POL√çGONOS: Genera trazas vectoriales clasificadas y almacena
    # sus segmentos en un set para filtrar la capa base
    # --------------------------------------------------------------------------------
    print("Generando capas vectoriales para sub-pol√≠gonos clasificados...")
    pattern_colors = {
        "cul_de_sac":   "red",
        "gridiron":     "green",
        "organico":     "blue",
        "hibrido":      "orange"
    }
    default_color = "gray"
    classification_traces = []
    classified_edges_set = set()

    for idx, row in gdf.iterrows():
        geom = row.geometry
        if geom is None or geom.is_empty:
            continue
        if geom.geom_type == "Polygon":
            polys = [geom]
        elif geom.geom_type == "MultiPolygon":
            polys = list(geom.geoms)
        else:
            continue

        for sub_idx, poly in enumerate(polys):
            key = (idx, sub_idx)
            if key in stats_dict:
                poly_stats = stats_dict[key]
                poly_class = classify_func(poly_stats)
            else:
                poly_class = None
            color = pattern_colors.get(poly_class, default_color)
            try:
                G_sub = ox.graph_from_polygon(poly, network_type=network_type, simplify=simplify)
            except Exception:
                continue
            if len(G_sub.edges()) == 0:
                continue
            G_sub = ox.project_graph(G_sub)
            sub_nodes = list(G_sub.nodes())
            sub_positions = np.array([(G_sub.nodes[n]['y'], G_sub.nodes[n]['x']) for n in sub_nodes])
            if sub_positions.size == 0:
                continue

            # Actualizar bounding box global
            lx_min, lx_max = sub_positions[:,1].min(), sub_positions[:,1].max()
            ly_min, ly_max = sub_positions[:,0].min(), sub_positions[:,0].max()
            global_x_min = min(global_x_min, lx_min)
            global_x_max = max(global_x_max, lx_max)
            global_y_min = min(global_y_min, ly_min)
            global_y_max = max(global_y_max, ly_max)

            # Procesar aristas del sub-grafo y almacenarlas en el set
            edge_x = []
            edge_y = []
            for u, v in G_sub.edges():
                if u in sub_nodes and v in sub_nodes:
                    u_idx = sub_nodes.index(u)
                    v_idx = sub_nodes.index(v)
                    # Extraer coordenadas de cada extremo (en proyecci√≥n)
                    x0 = sub_positions[u_idx][1]
                    y0 = sub_positions[u_idx][0]
                    x1 = sub_positions[v_idx][1]
                    y1 = sub_positions[v_idx][0]
                    # Normalizar el segmento
                    norm_edge = normalize_edge(x0, y0, x1, y1)
                    classified_edges_set.add(norm_edge)
                    edge_x.extend([x0, x1, None])
                    edge_y.extend([y0, y1, None])
                    
            edge_trace = go.Scattergl(
                x=edge_x, y=edge_y,
                mode='lines',
                line=dict(width=1.0, color=color),
                hoverinfo='none',
                name=f"Pol√≠gono {idx}-{sub_idx} ({poly_class})" if poly_class else f"Pol√≠gono {idx}-{sub_idx}"
            )
            x_sub = sub_positions[:,1]
            y_sub = sub_positions[:,0]
            node_trace = go.Scattergl(
                x=x_sub, y=y_sub,
                mode='markers',
                marker=dict(
                    size=1,
                    color=color,
                    opacity=0.9,
                    line=dict(width=0.4, color='black')
                ),
                hoverinfo='none',
                name=f"Pol√≠gono {idx}-{sub_idx} ({poly_class})" if poly_class else f"Pol√≠gono {idx}-{sub_idx}"
            )
            classification_traces.append(edge_trace)
            classification_traces.append(node_trace)

    # --------------------------------------------------------------------------------
    # C) CAPA BASE VECTORIAL: Filtrar segmentos que aparecen en la capa clasificada
    # --------------------------------------------------------------------------------
    print("Filtrando segmentos repetidos en la capa base...")
    base_edges_filtered = []
    base_edge_x = []
    base_edge_y = []
    for u, v in G_full.edges():
        if u in base_nodes and v in base_nodes:
            u_idx = base_nodes.index(u)
            v_idx = base_nodes.index(v)
            x0 = base_positions[u_idx][1]
            y0 = base_positions[u_idx][0]
            x1 = base_positions[v_idx][1]
            y1 = base_positions[v_idx][0]
            norm_edge = normalize_edge(x0, y0, x1, y1)
            if norm_edge not in classified_edges_set:
                # Este segmento no est√° en la capa clasificada, lo a√±adimos
                base_edge_x.extend([x0, x1, None])
                base_edge_y.extend([y0, y1, None])
                base_edges_filtered.append(norm_edge)
    
    base_edge_trace = go.Scattergl(
        x=base_edge_x, y=base_edge_y,
        mode='lines',
        line=dict(width=0.5, color='rgba(150,150,150,0.6)'),
        hoverinfo='none',
        name='BASE: Red completa'
    )

    # --------------------------------------------------------------------------------
    # D) Crear la figura final: Unir la capa base filtrada y las capas de clasificaci√≥n
    # --------------------------------------------------------------------------------
    final_traces = [base_edge_trace] + classification_traces

    x_range = [global_x_min, global_x_max]
    y_range = [global_y_min, global_y_max]

    layout = go.Layout(
        title=f'Clasificaci√≥n de Street Patterns - {place_name}',
        showlegend=True,
        hovermode='closest',
        margin=dict(b=20, l=20, r=20, t=40),
        xaxis=dict(
            range=x_range,
            autorange=False,
            scaleanchor="y",
            constrain='domain',
            showgrid=False,
            zeroline=False,
            showticklabels=False
        ),
        yaxis=dict(
            range=y_range,
            autorange=False,
            showgrid=False,
            zeroline=False,
            showticklabels=False
        ),
        template='plotly_white',
        height=800,
        paper_bgcolor="#F5F5F5",
        plot_bgcolor="#FFFFFF"
    )

    fig = go.Figure(data=final_traces, layout=layout)

    config = {
        'scrollZoom': True,
        'displayModeBar': True,
        'modeBarButtonsToRemove': ['select2d', 'lasso2d'],
        'toImageButtonOptions': {
            'format': 'svg',
            'width': 1600,
            'height': 1600
        },
        'responsive': True
    }

    os.makedirs(output_folder, exist_ok=True)
    out_html = os.path.join(output_folder, f"StreetPatterns_{place_name}.html")
    fig.write_html(
        out_html, 
        config=config, 
        include_plotlyjs='cdn', 
        auto_open=False, 
        full_html=True,
        default_width='100%',
        default_height='100%'
        )

    print(f"Archivo HTML generado: {out_html}")
    print("Capa base filtrada (sin duplicados) + sub-pol√≠gonos clasificados superpuestos.")

# # ================== EJEMPLO DE USO ==================
# if __name__ == "__main__":
#     # Define las rutas a tus archivos
#     geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA.geojson"
#     stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
#     stats_dict = load_polygon_stats_from_txt(stats_txt)
    

#     plot_street_patterns_classification(
#         geojson_path=geojson_file,
#         classify_func=classify_polygon,
#         stats_dict=stats_dict,
#         place_name="Medellin",
#         network_type="drive",
#         output_folder="Graphs_Cities",
#         simplify=False
#     )

def match_polygons_by_area(gdfA, gdfB, area_ratio_threshold=0.9, out_csv=None):
    """
    Empareja cada pol√≠gono de gdfA con el pol√≠gono de gdfB que m√°s
    √°rea de intersecci√≥n tiene (area(A ‚à© B) / area(A)).
    
    - Se asume que gdfA y gdfB tienen la misma proyecci√≥n (CRS).
    - Para evitar colisiones con la geometr√≠a de B, clonamos la geometr√≠a
      de B en una columna 'geomB' antes del sjoin.
    - Al final, generamos un DataFrame con (indexA, indexB, area_ratio).
    - Si area_ratio < area_ratio_threshold no se considera match.
    
    Par√°metros:
    -----------
    gdfA, gdfB : GeoDataFrames
        Con geometr√≠as Polygons
    area_ratio_threshold : float
        Umbral m√≠nimo para considerar un match.
    out_csv : str | None
        Ruta para guardar CSV, o None si no se desea.
    
    Retorna:
    --------
    DataFrame con columnas:
      - indexA : √≠ndice de pol√≠gono A
      - indexB : √≠ndice de pol√≠gono B
      - area_ratio : fracci√≥n del √°rea de A que se superpone con B
    """

    # Copias locales (para no alterar los gdf originales)
    gdfA = gdfA.copy()
    gdfB = gdfB.copy()

    # Asegurar √≠ndices para rastreo
    gdfA["indexA"] = gdfA.index
    gdfB["indexB"] = gdfB.index

    # 1. Clonar la geometr√≠a de B en una columna normal 'geomB'
    #    para no depender de c√≥mo sjoin renombra la geometry de B
    gdfB["geomB"] = gdfB.geometry

    # 2. Realizar sjoin con intersects
    #    - geometry principal en B sigue siendo 'geometry'
    #    - sjoin usar√° esa geometry para la intersecci√≥n
    joined = gpd.sjoin(
        gdfA,
        gdfB.drop(columns="geomB"),  # geometry nativa para sjoin
        how="inner",
        predicate="intersects",
        lsuffix="_A",
        rsuffix="_B"
    )

    # 'joined' tiene 'geometry' = la de A
    # y las columnas de B, excepto geomB (porque la droppeamos).
    # Sin embargo, conservamos 'geomB' en gdfB para reasignarla tras el sjoin

    # 3. Unir la columna 'geomB' de B a joined manualmente:
    #    Cada fila de joined tiene un 'indexB' que indica qu√© pol√≠gono de B era.
    #    Podemos hacer un merge con gdfB[["indexB","geomB"]] (en memory).
    joined = joined.merge(
        gdfB[["indexB", "geomB"]],
        on="indexB",
        how="left"
    )

    # 4. Calcular area_ratio = area(A ‚à© geomB) / area(A)
    def compute_area_ratio(row):
        geomA = row["geometry"]   # pol√≠gono de A
        geomB_ = row["geomB"]     # pol√≠gono original de B
        if geomA is None or geomB_ is None:
            return 0.0
        inter = geomA.intersection(geomB_)
        if inter.is_empty:
            return 0.0
        areaA = geomA.area
        if areaA == 0:
            return 0.0
        return inter.area / areaA

    joined["area_ratio"] = joined.apply(compute_area_ratio, axis=1)

    # 5. Para cada indexA, quedarnos con el pol√≠gono B de mayor area_ratio
    best_matches = joined.loc[
        joined.groupby("indexA")["area_ratio"].idxmax()
    ].copy()

    # 6. Filtrar por el umbral
    best_matches = best_matches[best_matches["area_ratio"] >= area_ratio_threshold]

    # 7. Extraer un DataFrame con indexA, indexB, area_ratio
    df_out = best_matches[["indexA", "indexB", "area_ratio"]].reset_index(drop=True)

    # 8. Guardar CSV si se desea
    if out_csv:
        df_out.to_csv(out_csv, index=False)
        print(f"Guardado {out_csv} con {len(df_out)} matches. Umbral={area_ratio_threshold}")

    return df_out

# # ==================== EJEMPLO DE USO ====================
# if __name__ == "__main__":

#     shpA = "Poligonos_Medellin/EOD_2017_SIT_only_AMVA.shp"
#     shpB = "Poligonos_Medellin/eod_gen_trips_mode.shp"

#     print("Leyendo shapefiles A y B...")
#     gdfA = gpd.read_file(shpA)
#     gdfB = gpd.read_file(shpB)

#     # Asegurar mismo CRS
#     if gdfA.crs != gdfB.crs:
#         gdfB = gdfB.to_crs(gdfA.crs)
#         print(f"Reproyectado B a {gdfA.crs}")

#     # Llamar funci√≥n con threshold=0.9
#     print("Iniciando match_polygons_by_area con area_ratio_threshold=0.9")
#     df_matches = match_polygons_by_area(
#         gdfA,
#         gdfB,
#         area_ratio_threshold=0.9,
#         out_csv="Poligonos_Medellin/Resultados/Matchs_A_B/matches_by_area.csv"
#     )

#     print("Algunos matches:")
#     print(df_matches.head())
#     print(f"Total matches: {len(df_matches)}")

def process_mobility_data(area_type='urban_and_rural'):
    # Configurar rutas y opciones seg√∫n el tipo de √°rea
    if area_type == 'urban':
        a_path = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
        include_absolute = False
        output_xlsx = "Poligonos_Medellin/Resultados/Statics_Results/URBAN/Poligonos_Clasificados_Movilidad_URBANO.xlsx"
        output_image = "Poligonos_Medellin/Resultados/Statics_Results/URBAN/map_poligonosA_urbano_classified.png"
    else:
        a_path = "Poligonos_Medellin/EOD_2017_SIT_only_AMVA.shp"
        include_absolute = False
        output_xlsx = "Poligonos_Medellin/Resultados/Statics_Results/URBAN_AND_RURAL/Poligonos_Clasificados_Movilidad_Urban_and_Rural.xlsx"
        output_image = "Poligonos_Medellin/Resultados/Statics_Results/URBAN_AND_RURAL/map_poligonosA_classified_Urban_and_Rural.png"
    
    # Rutas comunes
    stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
    matches_csv = "Poligonos_Medellin/Resultados/Matchs_A_B/matches_by_area.csv"
    shpB = "Poligonos_Medellin/eod_gen_trips_mode.shp"
    
    # 1) Cargar estad√≠sticas
    stats_dict = load_polygon_stats_from_txt(stats_txt)
    print(f"Cargadas stats para {len(stats_dict)} pol√≠gonos (subpol√≠gonos).")
    
    # 2) Cargar emparejamientos
    df_matches = pd.read_csv(matches_csv)
    print("Muestra df_matches:\n", df_matches.head(), "\n")
    
    # 3) Leer shapefile B (movilidad)
    gdfB = gpd.read_file(shpB)
    print("Columnas B:", gdfB.columns)
    
    # 4) Leer GeoDataFrame A (geometr√≠a)
    gdfA = gpd.read_file(a_path)
    print(f"Le√≠dos {len(gdfA)} pol√≠gonos en {'GeoJSON URBANO' if area_type == 'urban' else 'SHP'}.")
    
    # 5) Construir DataFrame final
    final_rows = []
    for _, row in df_matches.iterrows():
        idxA = row["indexA"]
        idxB = row["indexB"]
        ratio = row["area_ratio"]
        
        # Obtener estad√≠sticas y clasificar patr√≥n
        key_stats = (idxA, 0)
        poly_stats = stats_dict.get(key_stats, {})
        pattern = classify_polygon(poly_stats)
        
        # Extraer datos de movilidad
        rowB = gdfB.loc[idxB]
        p_walk_ = rowB.get("p_walk", 0)
        p_tpc_ = rowB.get("p_tpc", 0)
        p_sitva_ = rowB.get("p_sitva", 0)
        p_auto_ = rowB.get("p_auto", 0)
        p_moto_ = rowB.get("p_moto", 0)
        p_taxi_ = rowB.get("p_taxi", 0)
        p_bike_ = rowB.get("p_bike", 0)
        
        # Datos base para la fila
        row_data = {
            "indexA": idxA,
            "indexB": idxB,
            "area_ratio": ratio,
            "street_pattern": pattern,
            "p_walk": p_walk_,
            "p_tpc": p_tpc_,
            "p_sitva": p_sitva_,
            "p_auto": p_auto_,
            "p_moto": p_moto_,
            "p_taxi": p_taxi_,
            "p_bike": p_bike_
        }
        
        # Agregar datos absolutos si corresponde
        if include_absolute:
            row_data.update({
                "Auto": rowB.get("Auto", 0),
                "Moto": rowB.get("Moto", 0),
                "Taxi": rowB.get("Taxi", 0)
            })
        
        final_rows.append(row_data)
    
    # Definir columnas del DataFrame final
    columns = ["indexA", "indexB", "area_ratio", "street_pattern"]
    if include_absolute:
        columns += ["Auto", "Moto", "Taxi"]
    columns += ["p_walk", "p_tpc", "p_sitva", "p_auto", "p_moto", "p_taxi", "p_bike"]
    
    df_final = pd.DataFrame(final_rows)[columns]
    
    # Guardar Excel
    df_final.to_excel(output_xlsx, index=False)
    print(f"Guardado Excel final en {output_xlsx} con {len(df_final)} filas.\n")
    
# 6) Graficar pol√≠gonos clasificados
    gdfA["pattern"] = gdfA.index.map(df_final.set_index("indexA")["street_pattern"])
    
    # Mapeo de patrones a colores espec√≠ficos
    color_mapping = {
        'gridiron': 'Green',
        'cul_de_sac': 'Red',
        'hibrido': 'Blue',
        'organico': 'Yellow'
    }
    
    # Convertir a categor√≠as ordenadas
    categories = ['gridiron', 'cul_de_sac', 'hibrido', 'organico']
    gdfA['pattern'] = pd.Categorical(gdfA['pattern'], categories=categories)
    
    # Crear colormap personalizado
    cmap = ListedColormap([color_mapping[cat] for cat in categories])
    
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # Graficar con bordes oscuros
    gdfA.plot(
        column="pattern",
        ax=ax,
        legend=True,
        cmap=cmap,
        edgecolor='black',  # Borde negro
        linewidth=1,      # Grosor del borde
    )

    
    # Personalizar leyenda
    legend = ax.get_legend()
    legend.set_title('Patr√≥n de Calles')
    legend.set_bbox_to_anchor((1.05, 1))  # Mover leyenda a la derecha

    title = "Pol√≠gonos A clasificados" if area_type != 'urban' else "GeoJSON Urbano - Pol√≠gonos Clasificados"
    gdfA.plot(column="pattern", ax=ax, legend=True, cmap="Set2")
    ax.set_title(title)
    plt.tight_layout()
    plt.savefig(output_image, dpi=300)
    print(f"Mapa guardado en {output_image}")

# # Ejemplo de uso
# if __name__ == "__main__":
#     # Procesar datos urbanos y rurales
#     process_mobility_data(area_type='urban_and_rural')
    
#     # Procesar solo datos urbanos
#     process_mobility_data(area_type='urban')

def Statis_analisis(excel_file):
    # 1) Leer el Excel
    
    # Ajusta el nombre si tu archivo es diferente
    df = pd.read_excel(excel_file)
    # Esperamos cols: 
    # [indexA, indexB, area_ratio, street_pattern, p_walk, p_tpc, p_sitva, p_auto, p_moto, p_taxi, p_bike]
    print("Columnas df:", df.columns.tolist(), "\n")
    print(df.head(), "\n")

    # 2) Exploraci√≥n inicial
    # 2a) Cu√°ntos pol√≠gonos hay en cada Street Pattern:
    print("=== Conteo por Street Pattern ===")
    print(df["street_pattern"].value_counts())
    print()

    # 2b) Estad√≠sticos descriptivos por street_pattern
    mobility_cols = ["p_walk","p_tpc","p_sitva","p_auto","p_moto","p_taxi","p_bike"]
    desc = df.groupby("street_pattern")[mobility_cols].describe().T
    print("=== Estad√≠sticos descriptivos de las proporciones, por Street Pattern ===")
    print(desc)
    print()

    # 3) ANOVA / Kruskal-Wallis por variable
    #    Queremos ver si p_auto (por ej.) difiere segun street_pattern
    def anova_test(values_by_group):
        # values_by_group => lista de Series, una por cada StreetPattern
        return f_oneway(*values_by_group)

    def kruskal_test(values_by_group):
        return kruskal(*values_by_group)

    patterns = df["street_pattern"].unique()
    patterns_str = ", ".join(patterns)
    print(f"Street Patterns en el dataset: {patterns_str}\n")

    print("=== ANOVA / Kruskal para cada proporci√≥n de movilidad ===")
    for col in mobility_cols:
        groups = []
        for p in patterns:
            vals = df.loc[df["street_pattern"] == p, col].dropna()
            groups.append(vals)
        # ANOVA
        f_val, p_anova = anova_test(groups)
        # Kruskal
        h_val, p_kruskal = kruskal_test(groups)
        print(f"> {col.upper()} => ANOVA F={f_val:.3f} p={p_anova:.5g}, Kruskal H={h_val:.3f} p={p_kruskal:.5g}")
    print()

    # 4) Boxplots para visual
    for col in mobility_cols:
        sns.boxplot(x="street_pattern", y=col, data=df)
        plt.title(f"Distribuci√≥n de {col} por Street Pattern")
        plt.tight_layout()
        plt.savefig(f"boxplot_{col}.png", dpi=150)
        plt.close()
    print("Boxplots guardados (boxplot_{variable}.png).")

    # 5) Correlaci√≥n con One-Hot Encoding
    #    Creamos dummies => pattern_cul_de_sac, pattern_gridiron, etc.
    dummies = pd.get_dummies(df["street_pattern"], prefix="pattern")
    # Unimos con mobility_cols
    corr_df = pd.concat([df[mobility_cols], dummies], axis=1)
    # Spearman correlation
    corr_matrix = corr_df.corr(method="spearman")

    print("=== Matriz de correlaci√≥n (Spearman) entre proporciones y dummies de Street Pattern ===")
    print(corr_matrix, "\n")

    # Heatmap
    plt.figure(figsize=(9,7))
    sns.heatmap(corr_matrix, annot=True, cmap="RdBu", vmin=-1, vmax=1)
    plt.title("Matriz de Correlaci√≥n (Spearman)")
    plt.tight_layout()
    plt.savefig("heatmap_correlation.png", dpi=150)
    plt.close()
    print("Heatmap de correlaciones guardado en heatmap_correlation.png\n")

    # 6) REGRESI√ìN LOG√çSTICA MULTINOMIAL
    #    Podr√≠amos modelar street_pattern como variable dependiente,
    #    y p_walk, p_auto, etc. como predictores.
    #    Ejemplo con statsmodels MNLogit.
    #    Convertimos street_pattern a categorical codes 0,1,2,3
    df["pattern_code"] = df["street_pattern"].astype("category").cat.codes
    # Por ej. cul_de_sac=0, gridiron=1, organico=2, hibrido=3 (orden alfab√©tico)

    # X = p_walk, p_auto, etc.
    X = df[mobility_cols]
    y = df["pattern_code"]

    # statsmodels MNLogit requiere agregarnos una constante
    X_ = sm.add_constant(X, prepend=True)
    # Ajuste
    mnlogit_model = sm.MNLogit(y, X_)
    mnlogit_result = mnlogit_model.fit(method='newton', maxiter=100, disp=0)

    print("=== Regresi√≥n Log√≠stica Multinomial (Street Pattern ~ proporciones) ===")
    print(mnlogit_result.summary())
    print("\nInterpretaci√≥n: Coeficientes que comparan cada categor√≠a vs la base (cul_de_sac si code=0).")
    print("p-values indican si la proporci√≥n p_auto, etc. es significativa para distinguir el pattern.\n")

# if __name__ == "__main__":
#     excel_file = "Poligonos_Medellin/Resultados/Statics_Results/URBAN_AND_RURAL/Poligonos_Clasificados_Movilidad_Urban_and_Rural.xlsx"
#     excel_file = "Poligonos_Medellin/Resultados/Statics_Results/URBAN/Poligonos_Clasificados_Movilidad_URBANO.xlsx"
#    Statis_analisis(excel_file)

def filter_periphery_polygons(in_geojson, out_geojson, area_threshold=5.0):
    """
    Lee un GeoJSON (in_geojson), elimina pol√≠gonos con √°rea >= area_threshold (km¬≤),
    y guarda un nuevo GeoJSON en out_geojson con los pol√≠gonos filtrados.
    Retorna un GeoDataFrame con el resultado.

    Par√°metros:
    -----------
    in_geojson : ruta al archivo GeoJSON original.
    out_geojson: ruta donde se guardar√° el GeoJSON filtrado.
    area_threshold: float, umbral de √°rea en km¬≤; 
                    los pol√≠gonos con √°rea >= threshold se considerar√°n "rurales" y se excluyen.

    Retorna:
    --------
    GeoDataFrame con los pol√≠gonos ‚Äúurbanos‚Äù (√°rea < area_threshold).
    """

    # 1. Cargar el GeoDataFrame
    gdf = gpd.read_file(in_geojson)
    print(f"Le√≠do: {in_geojson} con {len(gdf)} pol√≠gonos totales.")

    # 2. Reproyectar a un sistema m√©trico para calcular √°rea en km¬≤ (por ejemplo EPSG:3395 o 3857)
    #    EPSG:3395 (World Mercator) o 3857 (Pseudo Mercator). Ajusta seg√∫n tu regi√≥n si deseas mayor precisi√≥n.
    gdf_merc = gdf.to_crs(epsg=3395)

    # 3. Calcular √°rea en km¬≤
    gdf["area_km2"] = gdf_merc.geometry.area / 1e6

    # 4. Filtrar
    mask_urban = gdf["area_km2"] < area_threshold
    gdf_filtered = gdf[mask_urban].copy()
    print(f"Se excluyen {len(gdf) - len(gdf_filtered)} pol√≠gonos por ser >= {area_threshold} km¬≤.")

    # 5. Guardar como GeoJSON nuevo
    #    (si no deseas la columna "area_km2" en el resultado, la dropeas antes)
    gdf_filtered.drop(columns=["area_km2"], inplace=True)
    gdf_filtered.to_file(out_geojson, driver="GeoJSON")
    print(f"Archivo filtrado guardado en: {out_geojson} con {len(gdf_filtered)} pol√≠gonos.\n")

    return gdf_filtered


# gdf_filtrado = filter_periphery_polygons(
#     in_geojson="Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA.geojson",
#     out_geojson="Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson",
#     area_threshold=5.0  # Ajusta a tu criterio
# )


# # Graph of visualization of medellin with its respective polygons
# geojson_file_filtered = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson "
# plot_road_network_from_geojson(geojson_file_filtered, network_type='drive', simplify=True)






#  =============================================================================
# ------ CALCULO PARA MALLA SIMPLIFICADA DE AREA URBANA MEDELLIN ANTIOQUIA -----------
#  =============================================================================

# # Ejemplo de uso
# if __name__ == "__main__":
#     geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
#     stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"

#     # 1. Cargar stats desde .txt
#     stats_dict = load_polygon_stats_from_txt(stats_txt)

#     # 2. Generar PNG con cada fila = (idx, 0)
#     plot_polygons_classification_png(
#         geojson_path=geojson_file,
#         stats_dict=stats_dict,
#         classify_func=classify_polygon,
#         output_png="polygon_classification_URBAN_MESH.png"
#     )


# # ================== EJEMPLO DE USO ==================
# if __name__ == "__main__":
#     # Define las rutas a tus archivos
#     geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
#     stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
#     stats_dict = load_polygon_stats_from_txt(stats_txt)
    

#     plot_street_patterns_classification(
#         geojson_path=geojson_file,
#         classify_func=classify_polygon,
#         stats_dict=stats_dict,
#         place_name="Medellin",
#         network_type="drive",
#         output_folder="Graphs_Cities",
#         simplify=False
#     )













def prepare_mobility_data(
    stats_txt="Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt", 
    matches_csv="Poligonos_Medellin/Resultados/Matchs_A_B/matches_by_area.csv", 
    shpB="Poligonos_Medellin/eod_gen_trips_mode.shp", 
    geojsonA="Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
 ):
    """
    Prepara un DataFrame de movilidad a partir de m√∫ltiples fuentes de datos.
    
    Par√°metros:
    -----------
    stats_txt : str, ruta al archivo de estad√≠sticas de pol√≠gonos
    matches_csv : str, ruta al CSV de emparejamientos A-B
    shpB : str, ruta al shapefile de movilidad
    geojsonA : str, ruta al GeoJSON de pol√≠gonos urbanos
    
    Retorna:
    --------
    DataFrame con datos de movilidad y caracter√≠sticas de pol√≠gonos
    """
    # 1) Cargar stats de pol√≠gonos
    stats_dict = load_polygon_stats_from_txt(stats_txt)
    print(f"Cargadas stats para {len(stats_dict)} pol√≠gonos (subpol√≠gono).")

    # 2) Cargar CSV de emparejamientos A-B
    df_matches = pd.read_csv(matches_csv)  # [indexA, indexB, area_ratio]
    print("Muestra df_matches:\n", df_matches.head(), "\n")

    # 3) Leer shapefile/GeoDataFrame B (movilidad)
    gdfB = gpd.read_file(shpB)
    print("Columnas B:", gdfB.columns)

    # 4) Leer GeoJSON A "URBANO"
    gdfA = gpd.read_file(geojsonA)
    print(f"Le√≠dos {len(gdfA)} pol√≠gonos en GeoJSON A URBANO.")

    # 5) Armar DataFrame final
    final_rows = []
    for i, row in df_matches.iterrows():
        idxA = row["indexA"]
        idxB = row["indexB"]
        ratio = row["area_ratio"]

        # Obtener estad√≠sticas del pol√≠gono
        key_stats = (idxA, 0)
        poly_stats = stats_dict.get(key_stats, {})
        pattern = classify_polygon(poly_stats)

        # Extraer movilidad de B
        rowB = gdfB.loc[idxB]

        # Variables proporcionales de movilidad
        mobility_columns = [
            "p_walk", "p_tpc", "p_sitva", 
            "p_auto", "p_moto", "p_taxi", "p_bike"
        ]
        
        mobility_data = {col: rowB.get(col, 0) for col in mobility_columns}

        # Construir fila de datos
        row_data = {
            "indexA": idxA,
            "indexB": idxB,
            "area_ratio": ratio,
            "street_pattern": pattern,
            **mobility_data
        }

        final_rows.append(row_data)

    # Crear DataFrame final
    df_final = pd.DataFrame(final_rows)
    df_final = df_final[[
        "indexA", "indexB", "area_ratio", "street_pattern", 
        "p_walk", "p_tpc", "p_sitva", 
        "p_auto", "p_moto", "p_taxi", "p_bike"
    ]]

    # Guardar como CSV
    output_path = "Poligonos_Medellin/Resultados/mobility_data.csv"
    df_final.to_csv(output_path, index=False)
    print(f"Datos de movilidad guardados en {output_path}")

    return df_final

def enhanced_polygon_clustering_visualization(df_merged, geojson_file, mobility_data):
    """
    Realiza visualizaciones de clusters de pol√≠gonos utilizando match_polygons_by_area.
    """
    # Ruta base para guardar resultados
    clustering_dir = "Poligonos_Medellin/Resultados/disaggregated measures/clustering"
    os.makedirs(clustering_dir, exist_ok=True)
    
    # M√©tricas de movilidad
    mobility_metrics = [
        'p_walk', 'p_tpc', 'p_sitva', 'p_auto', 'p_moto', 'p_taxi', 'p_bike'
    ]
    
    # Cargar GeoJSON
    gdf = gpd.read_file(geojson_file)
    
    # Preparar diccionario para resultados
    clustering_geojsons = {}
    
    # Procesar cada m√©trica de movilidad
    for mobility_metric in mobility_metrics:
        # Columna de cluster
        cluster_column = f'cluster_{mobility_metric}'
        
        # Verificar que la columna exista
        if cluster_column not in df_merged.columns:
            print(f"ADVERTENCIA: No se encontr√≥ columna {cluster_column}")
            continue
        
        # Crear copia del GeoJSON
        gdf_clusters = gdf.copy()
        
        # Asegurar correspondencia correcta de pol√≠gonos
        cluster_map = {}
        for _, row in mobility_data.iterrows():
            indexA, indexB = row['indexA'], row['indexB']
            
            # Buscar el cluster correspondiente en df_merged
            cluster_value = df_merged[
                (df_merged['indexA'] == indexA) & 
                (df_merged[cluster_column].notna())
            ][cluster_column].values
            
            if len(cluster_value) > 0:
                cluster_map[indexB] = cluster_value[0]
        
        # Asignar clusters al GeoDataFrame
        gdf_clusters[cluster_column] = gdf_clusters.index.map(cluster_map)
        
        # Visualizaci√≥n de clusters
        plt.figure(figsize=(15, 10))
        
        # Plotear clusters
        gdf_clusters.plot(column=cluster_column, 
                           cmap='viridis', 
                           edgecolor='black', 
                           linewidth=0.5, 
                           legend=True, 
                           missing_kwds={'color': 'lightgrey'})
        
        plt.title(f'Clusters de Pol√≠gonos - {mobility_metric}')
        plt.axis('off')
        plt.tight_layout()
        
        # Guardar mapa de clusters
        plt.savefig(os.path.join(clustering_dir, f'polygon_clusters_map_{mobility_metric}.png'), 
                    dpi=300, 
                    bbox_inches='tight')
        plt.close()
        
        # Almacenar en diccionario
        clustering_geojsons[mobility_metric] = gdf_clusters
    
    return clustering_geojsons

def polygon_detailed_statistical_analysis(polygon_stats_dict, mobility_data, geojson_file):
    """
    Realiza un an√°lisis estad√≠stico detallado de pol√≠gonos usando m√©tricas originales.
    """
    # Ruta base para guardar resultados
    output_dir = "Poligonos_Medellin/Resultados/disaggregated measures"
    clustering_dir = os.path.join(output_dir, "clustering")
    os.makedirs(clustering_dir, exist_ok=True)

    # Preparar DataFrame con m√©tricas de pol√≠gonos
    polygon_metrics = []
    for (poly_id, subpoly), stats_dict in polygon_stats_dict.items():
        poly_metrics = stats_dict.copy()
        poly_metrics['poly_id'] = poly_id
        poly_metrics['subpoly'] = subpoly
        polygon_metrics.append(poly_metrics)
    
    df_polygon_metrics = pd.DataFrame(polygon_metrics)
    
    # Combinar m√©tricas de pol√≠gonos con datos de movilidad
    df_merged = pd.merge(df_polygon_metrics, mobility_data, left_on=['poly_id'], right_on=['indexA'])
    
    # M√©tricas de pol√≠gono para an√°lisis
    structural_metrics = [
        'n', 'm', 'k_avg', 'edge_length_total', 'edge_length_avg', 
        'streets_per_node_avg', 'intersection_count', 'street_length_total', 
        'street_segment_count', 'street_length_avg', 'circuity_avg', 
        'intersection_density_km2', 'street_density_km2', 'area_km2'
    ]
    
    # M√©tricas de movilidad
    mobility_metrics = [
        'p_walk', 'p_tpc', 'p_sitva', 'p_auto', 'p_moto', 'p_taxi', 'p_bike'
    ]
    
    # Inicializar diccionario para almacenar resultados de clustering
    all_clustering_results = {}
        
    for mobility_metric in mobility_metrics:
        # Preparar caracter√≠sticas para clustering
        clustering_features = structural_metrics + [mobility_metric]
        
        # Escalar todas las caracter√≠sticas
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df_merged[clustering_features])
        
        # K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42).fit(X_scaled)
        
        # Crear columna de cluster en df_merged
        cluster_column = f'cluster_{mobility_metric}'
        df_merged[cluster_column] = kmeans.labels_
        
        # Perfiles de movilidad por cluster
        cluster_mobility_profiles = df_merged.groupby(cluster_column)[mobility_metrics].mean()
        print(f"Perfiles de Movilidad por Cluster para {mobility_metric}:")
        print(cluster_mobility_profiles)
        
        # Visualizaci√≥n de clusters
        plt.figure(figsize=(10, 6))
        scatter_colors = ['blue', 'green', 'red', 'purple']
        for i in range(4):
            cluster_data = df_merged[df_merged[cluster_column] == i]
            plt.scatter(cluster_data['street_density_km2'], cluster_data[mobility_metric], 
                        label=f'Cluster {i}', color=scatter_colors[i], alpha=0.7)
        
        plt.xlabel('Densidad de Calles (km¬≤)')
        plt.ylabel(f'Proporci√≥n de Viajes en {mobility_metric}')
        plt.title(f'Clusters de Pol√≠gonos - {mobility_metric}')
        plt.legend()
        plt.tight_layout()
        
        # Guardar imagen de clustering
        plt.savefig(os.path.join(clustering_dir, f'polygon_clusters_{mobility_metric}.png'))
        plt.close()
        
        # Guardar datos de clusters
        df_merged[['poly_id', cluster_column] + structural_metrics + mobility_metrics].to_csv(
            os.path.join(clustering_dir, f'polygon_cluster_data_{mobility_metric}.csv'), 
            index=False
        )
        
        # Guardar perfiles de movilidad por cluster
        cluster_mobility_profiles.to_csv(
            os.path.join(clustering_dir, f'cluster_mobility_profiles_{mobility_metric}.csv')
        )
        
        # Almacenar resultados
        all_clustering_results[mobility_metric] = {
            'cluster_mobility_profiles': cluster_mobility_profiles,
            'cluster_labels': df_merged[cluster_column]
        }
    
    # Visualizaci√≥n final de clusters usando GeoJSON
    gdf_clusters = enhanced_polygon_clustering_visualization(
        df_merged, 
        geojson_file, 
        mobility_data
    )
    
    return {
        'all_clustering_results': all_clustering_results,
        'merged_dataframe': df_merged
    }

# # Ejemplo de uso 
# geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
# stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
# df_mobility = prepare_mobility_data()
# stats_dict = load_polygon_stats_from_txt(stats_txt)
# results = polygon_detailed_statistical_analysis(stats_dict, df_mobility,geojson_file)




def calculate_angle_features(G):
    """
    Calcula caracter√≠sticas basadas en √°ngulos de los segmentos viales
    """
    # Verificar si el grafo es v√°lido y tiene nodos con coordenadas
    if G is None or G.number_of_nodes() == 0:
        return 0, 0, 0, 0
    
    # Lista para almacenar √°ngulos en intersecciones
    intersection_angles = []
    
    # Contador de segmentos ortogonales (aproximadamente 90¬∞)
    ortho_segments = 0
    
    # Procesar cada nodo con m√°s de 2 conexiones (intersecciones)
    for node, degree in G.degree():
        if degree > 2:  # Es una intersecci√≥n
            neighbors = list(G.neighbors(node))
            if len(neighbors) > 1:
                # Verificar que el nodo actual y vecinos tienen coordenadas
                valid_coords = True
                for n in [node] + neighbors:
                    if 'x' not in G.nodes[n] or 'y' not in G.nodes[n]:
                        valid_coords = False
                        break
                
                if not valid_coords:
                    continue
                
                # Calcular √°ngulos entre segmentos conectados a esta intersecci√≥n
                node_angles = []
                for n1 in neighbors:
                    x1, y1 = G.nodes[node]['x'], G.nodes[node]['y']
                    x2, y2 = G.nodes[n1]['x'], G.nodes[n1]['y']
                    
                    # Calcular el √°ngulo del segmento respecto al eje horizontal
                    angle = degrees(atan2(y2 - y1, x2 - x1)) % 360
                    node_angles.append(angle)
                
                # Calcular √°ngulos entre pares de segmentos en esta intersecci√≥n
                for i in range(len(node_angles)):
                    for j in range(i+1, len(node_angles)):
                        # Calcular la diferencia angular
                        angle_diff = abs(node_angles[i] - node_angles[j]) % 180
                        # Normalizar para obtener el √°ngulo menor
                        if angle_diff > 90:
                            angle_diff = 180 - angle_diff
                        
                        intersection_angles.append(angle_diff)
                        
                        # Contar si es una intersecci√≥n aproximadamente ortogonal (90¬∞ ¬± 10¬∞)
                        if 80 <= angle_diff <= 100:
                            ortho_segments += 1
    
    # Evitar divisi√≥n por cero
    if not intersection_angles:
        return 0, 0, 0, 0
    
    # Caracter√≠sticas basadas en √°ngulos
    mean_angle = np.mean(intersection_angles)
    std_angle = np.std(intersection_angles)
    
    # Proporci√≥n de intersecciones ortogonales
    ortho_proportion = ortho_segments / len(intersection_angles) if len(intersection_angles) > 0 else 0
    
    # Coeficiente de variaci√≥n para √°ngulos (normaliza la desviaci√≥n est√°ndar)
    cv_angle = std_angle / mean_angle if mean_angle > 0 else 0
    
    return mean_angle, std_angle, ortho_proportion, cv_angle

def calculate_dead_end_features(G):
    """
    Calcula caracter√≠sticas relacionadas con calles sin salida y cul-de-sacs
    """
    # Verificar si el grafo es v√°lido
    if G is None or G.number_of_nodes() == 0:
        return 0, 0
    
    # Contar nodos de grado 1 (calles sin salida)
    dead_ends = [node for node, degree in G.degree() if degree == 1]
    dead_end_count = len(dead_ends)
    
    # Total de nodos
    total_nodes = G.number_of_nodes()
    
    # Proporci√≥n de calles sin salida
    dead_end_ratio = dead_end_count / total_nodes if total_nodes > 0 else 0
    
    # Analizar las distancias entre calles sin salida
    distances = []
    if len(dead_ends) > 1:
        for i, d1 in enumerate(dead_ends):
            if 'x' not in G.nodes[d1] or 'y' not in G.nodes[d1]:
                continue
                
            x1, y1 = G.nodes[d1]['x'], G.nodes[d1]['y']
            for d2 in dead_ends[i+1:]:
                if 'x' not in G.nodes[d2] or 'y' not in G.nodes[d2]:
                    continue
                    
                x2, y2 = G.nodes[d2]['x'], G.nodes[d2]['y']
                dist = np.sqrt((x2-x1)**2 + (y2-y1)**2)
                distances.append(dist)
    
    # Coeficiente de variaci√≥n de distancias entre calles sin salida
    if distances:
        mean_dist = np.mean(distances)
        if mean_dist > 0:
            cv_distances = np.std(distances) / mean_dist
        else:
            cv_distances = 0
    else:
        cv_distances = 0
    
    return dead_end_ratio, cv_distances

def procesar_poligonos_y_generar_grafos(gdf):
    """
    Procesa pol√≠gonos y genera grafos de red vial para cada uno.
    
    Args:
        gdf: GeoDataFrame con pol√≠gonos a procesar
        
    Returns:
        graph_dict: Diccionario con los grafos generados (clave: poly_id, valor: grafo)
    """
    print("Procesando pol√≠gonos y generando grafos de red vial...")
    graph_dict = {}  # Diccionario para almacenar los grafos
    
    # Verificar versi√≥n de OSMnx para determinar los par√°metros correctos
    print(f"Versi√≥n de OSMnx: {ox.__version__}")
    total_polygons = len(gdf)

    # Procesar cada pol√≠gono
    for idx, row in gdf.iterrows():
        

        try:
            poly_id = row['poly_id'] if 'poly_id' in gdf.columns else str(idx)
            geometry = row.geometry
            # print(f"Procesando pol√≠gono {idx+1}/{total_polygons} (ID: {poly_id})")
            # Asegurarnos de que el pol√≠gono es v√°lido
            if geometry is None or not geometry.is_valid:
                print(f"Pol√≠gono {poly_id} no v√°lido, omitiendo")
                continue
            
            # Intentar obtener la red vial del pol√≠gono
            try:
                # Obtener la red vial del pol√≠gono adapt√°ndonos a la versi√≥n de OSMnx
                try:
                    # Primero intentamos con el par√°metro clean_periphery
                    G = ox.graph_from_polygon(geometry, network_type='drive', simplify=True, clean_periphery=True)
                except TypeError:
                    # Si falla, probamos sin ese par√°metro
                    G = ox.graph_from_polygon(geometry, network_type='drive', simplify=True)
                
                # Si el grafo est√° vac√≠o, omitir este pol√≠gono
                if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
                    print(f"Pol√≠gono {poly_id} gener√≥ un grafo vac√≠o, omitiendo")
                    continue
                
                # A√±adir proyecci√≥n para asegurar que tenemos coordenadas x,y
                G = ox.project_graph(G)
                
                # Almacenar el grafo en el diccionario
                graph_dict[poly_id] = G
                
            except Exception as e:
                print(f"Error al obtener grafo para pol√≠gono {poly_id}: {e}")
                continue
                
        except Exception as e:
            print(f"Error general procesando pol√≠gono {idx}: {e}")
            continue

    print(f"Procesamiento completado. Se generaron {len(graph_dict)} grafos v√°lidos.")
    
    return graph_dict

def preprocess_to_dimensionless(X_array, feature_names):
    """
    Transforma todas las variables a formas adimensionales sin usar valores de referencia fijos,
    sino utilizando estad√≠sticas de los propios datos.
    
    Par√°metros:
    - X_array: Array NumPy con los datos originales
    - feature_names: Lista con los nombres de las caracter√≠sticas
    
    Retorna:
    - X_preprocessed: Array NumPy con los datos pre-procesados completamente adimensionales
    """
    
    # Crear una copia para no modificar los datos originales
    X_preprocessed = X_array.copy()
    
    # Mapeo de cada caracter√≠stica a su tipo para pre-procesamiento espec√≠fico
    feature_types = {
        # Densidades (con unidades km^-2)
        'edge_length_density': 'density',
        'street_density_km2': 'density',
        'node_density_km2': 'density',
        'edge_density_km2': 'density',
        'intersection_density_km2': 'density',
        'segment_density_km2': 'density',
        
        # Longitudes (con unidades de metros)
        'edge_length_avg': 'length',
        'street_length_avg': 'length',
        
        # Variables ya adimensionales
        'k_avg': 'dimensionless',
        'streets_per_node_avg': 'dimensionless',
        'network_connectivity_index': 'dimensionless',
        'circuity_avg': 'circuity',
        'orthogonal_proportion': 'dimensionless',
        'angle_coefficient_variation': 'dimensionless',
        'dead_end_ratio': 'dimensionless',
        'cv_dead_end_distances': 'dimensionless',
        
        # √Ångulos (con unidades de grados)
        'mean_intersection_angle': 'angle',
        'std_intersection_angle': 'angle_deviation'
    }
    
    # Separar variables por tipo
    density_indices = [i for i, name in enumerate(feature_names) if feature_types.get(name) == 'density']
    length_indices = [i for i, name in enumerate(feature_names) if feature_types.get(name) == 'length']
    angle_indices = [i for i, name in enumerate(feature_names) if feature_types.get(name) == 'angle']
    deviation_indices = [i for i, name in enumerate(feature_names) if feature_types.get(name) == 'angle_deviation']
    circuity_indices = [i for i, name in enumerate(feature_names) if feature_types.get(name) == 'circuity']
    
    # 1. Transformar densidades: dividir por la densidad m√°xima o mediana de cada tipo
    if density_indices:
        for i in density_indices:
            # Usar percentil 95 como referencia para evitar outliers extremos
            reference = np.percentile(X_preprocessed[:, i][X_preprocessed[:, i] > 0], 95)
            if reference > 0:
                X_preprocessed[:, i] = X_preprocessed[:, i] / reference
    
    # 2. Transformar longitudes: dividir por la longitud m√°xima o mediana
    if length_indices:
        for i in length_indices:
            # Usar percentil 95 como referencia para evitar outliers extremos
            reference = np.percentile(X_preprocessed[:, i][X_preprocessed[:, i] > 0], 95)
            if reference > 0:
                X_preprocessed[:, i] = X_preprocessed[:, i] / reference
    
    # 3. Transformar √°ngulos: expresar como fracci√≥n de un c√≠rculo completo
    if angle_indices:
        for i in angle_indices:
            X_preprocessed[:, i] = X_preprocessed[:, i] / 360.0
    
    # 4. Transformar desviaciones de √°ngulos: expresar como fracci√≥n de un √°ngulo recto
    if deviation_indices:
        for i in deviation_indices:
            X_preprocessed[:, i] = X_preprocessed[:, i] / 90.0
    
    # 5. Transformar circuidad: restar 1.0 para que el ideal sea 0
    if circuity_indices:
        for i in circuity_indices:
            X_preprocessed[:, i] = X_preprocessed[:, i] - 1.0
    
    return X_preprocessed

def prepare_clustering_features_improved(stats_dict, graph_dict):  
    """
    Prepara caracter√≠sticas para clustering con normalizaci√≥n por √°rea para medidas absolutas
    y conservaci√≥n de m√©tricas relativas, incluyendo caracter√≠sticas de √°ngulos y calles sin salida.
    
    Par√°metros:
    - stats_dict: Diccionario con estad√≠sticas por pol√≠gono
    - graph_dict: Diccionario con los grafos NetworkX por pol√≠gono (donde la clave es poly_id)
    """
      
    feature_names = [
        'edge_length_density',        # Longitud de enlaces por km¬≤ (normalizada)
        'street_density_km2',         # Longitud de calles por km¬≤ (ya normalizada)
        'node_density_km2',           # Densidad de nodos por km¬≤ (nueva)
        'edge_density_km2',           # Densidad de enlaces por km¬≤ (nueva)
        'k_avg',                      # Grado promedio (ya relativa)
        'edge_length_avg',            # Longitud promedio de enlaces (ya relativa)
        'streets_per_node_avg',       # Promedio de calles por nodo (ya relativa)
        'intersection_density_km2',   # Densidad de intersecciones por km¬≤ (ya normalizada)
        'segment_density_km2',        # Densidad de segmentos de calle por km¬≤ (nueva)
        'street_length_avg',          # Longitud promedio de calle (ya relativa)
        'circuity_avg',               # Circuidad promedio (ya relativa)
        'network_connectivity_index', # √çndice de conectividad (ya relativa)
        'mean_intersection_angle',    # √Ångulo promedio de intersecci√≥n (nueva)
        'std_intersection_angle',     # Desviaci√≥n est√°ndar de √°ngulos (nueva)
        'orthogonal_proportion',      # Proporci√≥n de √°ngulos ortogonales (nueva) 
        'angle_coefficient_variation',# Coeficiente de variaci√≥n de √°ngulos (nueva)
        'dead_end_ratio',             # Ratio de calles sin salida (nueva)
        'cv_dead_end_distances'       # Coef. de variaci√≥n de distancias entre calles sin salida (nueva)
    ]

       # Crear un diccionario para mapear entre los diferentes formatos de ID
    id_map = {}
    for graph_id in graph_dict.keys():
        # Si es un string, intentar convertirlo a entero para comparar
        if isinstance(graph_id, str) and graph_id.isdigit():
            id_map[int(graph_id)] = graph_id
        # A√±adir el ID original tambi√©n
        id_map[graph_id] = graph_id
    
    X = []
    poly_ids = []
    
    
    
    # Extraer caracter√≠sticas
    for poly_id, stats in stats_dict.items():
        try:
            feature_vector = []
            
            # Verificar que tenemos √°rea para normalizar
            area_km2 = stats.get('area_km2', 0)
            if area_km2 <= 0:
                print(f"Advertencia: √°rea no v√°lida para {poly_id}, omitiendo")
                continue
                
            # 1. edge_length_density (nueva: edge_length_total / area_km2)
            edge_length_total = stats.get('edge_length_total', 0)
            feature_vector.append(edge_length_total / area_km2)
            
            # 2. street_density_km2 (ya existe)
            feature_vector.append(stats.get('street_density_km2', 0))
            
            # 3. node_density_km2 (nueva: n / area_km2)
            n_nodes = stats.get('n', 0)
            feature_vector.append(n_nodes / area_km2)
            
            # 4. edge_density_km2 (nueva: m / area_km2)
            m_edges = stats.get('m', 0)
            feature_vector.append(m_edges / area_km2)
            
            # 5-7. M√©tricas relativas (se mantienen igual)
            feature_vector.append(stats.get('k_avg', 0))
            feature_vector.append(stats.get('edge_length_avg', 0))
            feature_vector.append(stats.get('streets_per_node_avg', 0))
            
            # 8. intersection_density_km2 (ya existe)
            feature_vector.append(stats.get('intersection_density_km2', 0))
            
            # 9. segment_density_km2 (nueva: street_segment_count / area_km2)
            segment_count = stats.get('street_segment_count', 0)
            feature_vector.append(segment_count / area_km2)
            
            # 10-11. M√©tricas relativas (se mantienen igual)
            feature_vector.append(stats.get('street_length_avg', 0))
            feature_vector.append(stats.get('circuity_avg', 0))
            
            # 12. network_connectivity_index (calcularlo seg√∫n tu c√≥digo original)
            # Calcular √≠ndice de conectividad a partir de streets_per_node_proportions o streets_per_node_counts
            connectivity_index = 0.0
            
            # Procesamiento de streets_per_node_proportions (mantener igual que en el c√≥digo original)
            if 'streets_per_node_proportions' in stats:
                # Convertir a diccionario si es string
                if isinstance(stats['streets_per_node_proportions'], str):
                    try:
                        streets_prop = ast.literal_eval(stats['streets_per_node_proportions'])
                    except:
                        # Si falla la conversi√≥n, intentamos con streets_per_node_counts
                        if 'streets_per_node_counts' in stats and isinstance(stats['streets_per_node_counts'], str):
                            try:
                                streets_counts = ast.literal_eval(stats['streets_per_node_counts'])
                                # Convertir counts a proportions
                                total_nodes = sum(streets_counts.values())
                                streets_prop = {k: v/total_nodes for k, v in streets_counts.items()} if total_nodes else {1: 0.3, 3: 0.6, 4: 0.1}
                            except:
                                streets_prop = {1: 0.3, 3: 0.6, 4: 0.1}  # Valores predeterminados
                        else:
                            streets_prop = {1: 0.3, 3: 0.6, 4: 0.1}  # Valores predeterminados
                else:
                    streets_prop = stats['streets_per_node_proportions']
            
            # Alternativa: Si no tenemos proportions pero s√≠ tenemos counts
            elif 'streets_per_node_counts' in stats:
                if isinstance(stats['streets_per_node_counts'], str):
                    try:
                        streets_counts = ast.literal_eval(stats['streets_per_node_counts'])
                        # Convertir counts a proportions
                        total_nodes = sum(streets_counts.values())
                        streets_prop = {k: v/total_nodes for k, v in streets_counts.items()} if total_nodes else {1: 0.3, 3: 0.6, 4: 0.1}
                    except:
                        streets_prop = {1: 0.3, 3: 0.6, 4: 0.1}  # Valores predeterminados
                else:
                    streets_counts = stats['streets_per_node_counts']
                    total_nodes = sum(streets_counts.values())
                    streets_prop = {k: v/total_nodes for k, v in streets_counts.items()} if total_nodes else {1: 0.3, 3: 0.6, 4: 0.1}
            else:
                # Si no tenemos ni proportions ni counts
                streets_prop = {1: 0.3, 3: 0.6, 4: 0.1}  # Valores predeterminados
            
            # Extraer proporciones por tipo de nodo
            dead_end_prop = streets_prop.get(1, 0.0)
            continuing_road_prop = streets_prop.get(2, 0.0)
            t_intersection_prop = streets_prop.get(3, 0.0)
            cross_intersection_prop = streets_prop.get(4, 0.0)
            
            # F√≥rmula para connectivity_index
            connectivity_index = (
                (1 * dead_end_prop) +
                (2 * continuing_road_prop) +
                (3 * t_intersection_prop) +
                (4 * cross_intersection_prop)
            ) / 4.0
            
            feature_vector.append(connectivity_index)
            
            # 13-18. Nuevas caracter√≠sticas de √°ngulos y calles sin salida
            # Obtener el grafo para este pol√≠gono
            G = None
            
            # Estrategia 1: Usar el ID directamente
            if poly_id in graph_dict:
                G = graph_dict[poly_id]
            
            # Estrategia 2: Si poly_id es una tupla, usar el primer elemento
            elif isinstance(poly_id, tuple) and len(poly_id) > 0:
                first_element = poly_id[0]
                if first_element in graph_dict:
                    G = graph_dict[first_element]
                elif str(first_element) in graph_dict:
                    G = graph_dict[str(first_element)]
                elif first_element in id_map:
                    G = graph_dict[id_map[first_element]]
            
            # Estrategia 3: Convertir a string
            elif str(poly_id) in graph_dict:
                G = graph_dict[str(poly_id)]
            
            if G is not None:
                # Calcular caracter√≠sticas de √°ngulos y calles sin salida
                mean_angle, std_angle, ortho_prop, cv_angle = calculate_angle_features(G)
                feature_vector.extend([mean_angle, std_angle, ortho_prop, cv_angle])
                
                dead_end_ratio, cv_dead_end = calculate_dead_end_features(G)
                feature_vector.extend([dead_end_ratio, cv_dead_end])
            else:
                # Si no tenemos grafo para este pol√≠gono, usar valores predeterminados
                print(f"Advertencia: No se encontr√≥ grafo para {poly_id}, usando valores predeterminados")
                feature_vector.extend([0, 0, 0, 0])  # √Ångulos
                feature_vector.extend([0, 0])  
                        
            # Verificar valores at√≠picos en todo el vector
            if any(np.isnan(val) or np.isinf(val) for val in feature_vector):
                print(f"Advertencia: valores at√≠picos detectados para {poly_id}, omitiendo")
                continue
                
            # Si llegamos hasta aqu√≠, a√±adimos el vector al conjunto de datos
            X.append(feature_vector)
            poly_ids.append(poly_id)
                
        except Exception as e:
            print(f"Error procesando {poly_id}: {e}")
            continue
    
    # Verificar que tenemos suficientes muestras
    if len(X) < 2:
        print(f"ADVERTENCIA: Solo se encontraron {len(X)} muestras v√°lidas para clustering.")
    else:
        print(f"Se prepararon {len(X)} muestras v√°lidas para clustering.")
    
    # Imprimir las caracter√≠sticas para verificaci√≥n
    print("Caracter√≠sticas utilizadas:", feature_names)
    

    # Dentro de tu funci√≥n, justo antes del return:
    X_array = np.array(X)

    # Aplicar pre-procesamiento para hacer todas las variables adimensionales
    X_preprocessed = preprocess_to_dimensionless(X_array, feature_names)
    
    # Luego aplicar StandardScaler para normalizaci√≥n estad√≠stica final
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X_preprocessed)


    return X_normalized, poly_ids, feature_names

def find_optimal_k_improved(X_scaled, max_k=10, min_k=2):
    """
    Encuentra el n√∫mero √≥ptimo de clusters usando silhouette score,
    calinski-harabasz index y modularity score (para redes).
    """
   
    
    results = []
    
    for k in range(min_k, max_k + 1):
        # Usar KMeans++ para mejor inicializaci√≥n
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='k-means++')
        labels = kmeans.fit_predict(X_scaled)
        
        # Calcular m√©tricas
        silhouette = silhouette_score(X_scaled, labels)
        calinski = calinski_harabasz_score(X_scaled, labels)
        
        # Construir grafo basado en similitudes de clustering
        G = nx.Graph()
        for i in range(len(labels)):
            G.add_node(i, cluster=labels[i])
        
        # Agregar conexiones basadas en proximidad de clusters
        for i in range(len(labels)):
            for j in range(i + 1, len(labels)):
                if labels[i] == labels[j]:
                    G.add_edge(i, j)
        
        communities = {c: set(np.where(labels == c)[0]) for c in set(labels)}.values()
        mod_score = modularity(G, communities)
        
        results.append({
            'k': k,
            'silhouette': silhouette,
            'calinski': calinski,
            'modularity': mod_score,
            'inertia': kmeans.inertia_
        })
        
        print(f"K={k}, Silhouette={silhouette:.4f}, Calinski-Harabasz={calinski:.1f}, Modularity={mod_score:.4f}")
    
    # Visualizar resultados
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))
    
    # Silhouette score (mayor es mejor)
    axs[0, 0].plot([r['k'] for r in results], [r['silhouette'] for r in results], 'o-', color='blue')
    axs[0, 0].set_xlabel('N√∫mero de clusters')
    axs[0, 0].set_ylabel('Silhouette Score')
    axs[0, 0].set_title('Silhouette Score (mayor es mejor)')
    
    # Calinski-Harabasz (mayor es mejor)
    axs[0, 1].plot([r['k'] for r in results], [r['calinski'] for r in results], 'o-', color='green')
    axs[0, 1].set_xlabel('N√∫mero de clusters')
    axs[0, 1].set_ylabel('Calinski-Harabasz Index')
    axs[0, 1].set_title('Calinski-Harabasz Index (mayor es mejor)')
    
    # Modularity Score (mayor es mejor)
    axs[1, 0].plot([r['k'] for r in results], [r['modularity'] for r in results], 'o-', color='red')
    axs[1, 0].set_xlabel('N√∫mero de clusters')
    axs[1, 0].set_ylabel('Modularity Score')
    axs[1, 0].set_title('Modularity Score (mayor es mejor)')
    
    # Inertia (m√©todo del codo)
    axs[1, 1].plot([r['k'] for r in results], [r['inertia'] for r in results], 'o-', color='purple')
    axs[1, 1].set_xlabel('N√∫mero de clusters')
    axs[1, 1].set_ylabel('Inertia')
    axs[1, 1].set_title('M√©todo del codo')
    
    plt.tight_layout()
    plt.savefig('Resultados/urbano_pattern_cluster/cluster_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Normalizar m√©tricas para combinarlas
    sil_norm = [r['silhouette'] / max([r['silhouette'] for r in results]) for r in results]
    cal_norm = [r['calinski'] / max([r['calinski'] for r in results]) for r in results]
    mod_norm = [r['modularity'] / max([r['modularity'] for r in results]) for r in results]
    
    # Calcular score combinado
    combined_scores = [(s + c + m) / 3 for s, c, m in zip(sil_norm, cal_norm, mod_norm)]
    
    # Encontrar k √≥ptimo por score combinado
    optimal_k_idx = np.argmax(combined_scores)
    optimal_k = results[optimal_k_idx]['k']
    
    print(f"\nK √≥ptimo seg√∫n score combinado: {optimal_k}")
    return optimal_k

# def optimal_clustering_improved(X, feature_names, n_clusters=None, use_pca=True, visualize=True):
#     """
#     Realiza clustering mejorado con KMeans y an√°lisis de caracter√≠sticas importantes
#     """
    
#     # Eliminar filas con NaN o infinitos
#     valid_rows = ~np.any(np.isnan(X) | np.isinf(X), axis=1)
#     X_clean = X[valid_rows]
#     if X_clean.shape[0] < X.shape[0]:
#         print(f"Eliminadas {X.shape[0] - X_clean.shape[0]} filas con valores no v√°lidos")
    
#     # Normalizar caracter√≠sticas
#     scaler = StandardScaler()
#     X_scaled = scaler.fit_transform(X_clean)
    
#     # Reducci√≥n de dimensionalidad
#     if use_pca:
#         # Determinar n√∫mero √≥ptimo de componentes (varianza explicada > 0.95)
#         full_pca = PCA().fit(X_scaled)
#         cum_var = np.cumsum(full_pca.explained_variance_ratio_)
#         n_components = np.argmax(cum_var >= 0.95) + 1
#         n_components = max(2, min(n_components, X_scaled.shape[1]))
        
#         pca = PCA(n_components=n_components)
#         X_reduced = pca.fit_transform(X_scaled)
        
#         # An√°lisis de componentes principales
#         print(f"\nAn√°lisis PCA con {n_components} componentes:")
#         print(f"Varianza explicada: {pca.explained_variance_ratio_}")
#         print(f"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}")
        
#         # Visualizar contribuci√≥n de caracter√≠sticas a componentes
#         plt.figure(figsize=(12, 8))
#         components = pd.DataFrame(
#             pca.components_.T,
#             columns=[f'PC{i+1}' for i in range(n_components)],
#             index=feature_names
#         )
        
#         sns.heatmap(components, cmap='coolwarm', annot=True, fmt=".2f")
#         plt.title('Contribuci√≥n de variables a componentes principales')
#         plt.tight_layout()
#         plt.savefig('Resultados/urbano_pattern_cluster/pca_components_contributions.png', dpi=300, bbox_inches='tight')
#         plt.close()
#     else:
#         X_reduced = X_scaled
    
#     # Encontrar n√∫mero √≥ptimo de clusters si no se proporciona
#     if n_clusters is None:
#         # Asumimos que la funci√≥n find_optimal_k_improved est√° definida en otro lugar
#         n_clusters = find_optimal_k_improved(X_reduced, max_k=8, min_k=3)
    
#     print(f"\nRealizando clustering KMeans con {n_clusters} clusters")
    
#     # Usar KMeans con inicializaci√≥n k-means++ y m√∫ltiples inicios
#     kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
#     cluster_labels = kmeans.fit_predict(X_reduced)
    
#     # Analizar centros de clusters
#     if use_pca:
#         # Proyectar centros al espacio original
#         centers_pca = kmeans.cluster_centers_
#         centers_original = pca.inverse_transform(centers_pca)
#         centers_original = scaler.inverse_transform(centers_original)
#     else:
#         centers_original = scaler.inverse_transform(kmeans.cluster_centers_)
    
#     # Crear DataFrame de centros
#     centers_df = pd.DataFrame(centers_original, columns=feature_names)
#     centers_df.index = [f'Cluster {i}' for i in range(n_clusters)]
    
#     print("\nCaracter√≠sticas de los centros de clusters:")
#     print(centers_df)
    
#     # Analizar variables m√°s discriminantes entre clusters
#     cluster_importance = {}
#     for feature in feature_names:
#         # Calcular varianza entre clusters para esta caracter√≠stica
#         values = centers_df[feature].values
#         variance = np.var(values)
#         max_diff = np.max(values) - np.min(values)
#         importance = variance * max_diff  # Ponderaci√≥n por rango
#         cluster_importance[feature] = importance
    
#     sorted_features = sorted(cluster_importance.items(), key=lambda x: x[1], reverse=True)
    
#     print("\nCaracter√≠sticas m√°s importantes para diferenciar clusters:")
#     for feature, importance in sorted_features[:5]:
#         print(f"{feature}: {importance:.4f}")
    
#     # Visualizar clusters
#     if visualize:
#         # CORRECCI√ìN: En lugar de usar t-SNE, que puede no preservar distancias globales,
#         # usar PCA para visualizaci√≥n si el n√∫mero de caracter√≠sticas es alto
#         if X_reduced.shape[1] > 2:
#             # Para visualizaci√≥n, usamos PCA directamente desde los datos escalados
#             viz_pca = PCA(n_components=2)
#             X_viz = viz_pca.fit_transform(X_scaled)
            
#             plt.figure(figsize=(10, 8))
#             scatter = plt.scatter(X_viz[:, 0], X_viz[:, 1], c=cluster_labels, 
#                                  cmap='viridis', s=50, alpha=0.8)
            
#             # Transformar centros de clusters a 2D para visualizaci√≥n
#             if use_pca:
#                 # Primero a espacio escalado
#                 centers_scaled = scaler.transform(centers_original)
#                 # Luego proyectar a 2D con el mismo PCA de visualizaci√≥n
#                 centers_viz = viz_pca.transform(centers_scaled)
#             else:
#                 centers_viz = viz_pca.transform(kmeans.cluster_centers_)
            
#             # Mostrar centros en la visualizaci√≥n
#             plt.scatter(centers_viz[:, 0], centers_viz[:, 1], 
#                        c='red', s=200, alpha=0.8, marker='X')
            
#             # A√±adir etiquetas de clusters
#             for i, (x, y) in enumerate(centers_viz):
#                 plt.annotate(f'Cluster {i}', (x, y), fontsize=12, 
#                              ha='center', va='center', color='white',
#                              bbox=dict(boxstyle="round,pad=0.3", fc='black', alpha=0.7))
            
#             plt.colorbar(scatter, label='Cluster')
#             plt.title('Visualizaci√≥n de clusters usando PCA')
#             plt.xlabel('PC1')
#             plt.ylabel('PC2')
#             plt.tight_layout()
#             plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_pca.png', dpi=300, bbox_inches='tight')
#             plt.close()
            
#             # Adicionalmente, podemos usar t-SNE como visualizaci√≥n complementaria
#             # pero con par√°metros m√°s adecuados
#             if X_clean.shape[0] > 5:  # Solo si hay suficientes datos
#                 perplexity = min(30, max(5, X_clean.shape[0] // 10))
#                 tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity,
#                            learning_rate='auto', init='pca')
#                 X_tsne = tsne.fit_transform(X_scaled)
                
#                 plt.figure(figsize=(10, 8))
#                 scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], 
#                                      c=cluster_labels, cmap='viridis', s=50, alpha=0.8)
                
#                 plt.colorbar(scatter, label='Cluster')
#                 plt.title('Visualizaci√≥n de clusters usando t-SNE')
#                 plt.xlabel('t-SNE 1')
#                 plt.ylabel('t-SNE 2')
#                 plt.tight_layout()
#                 plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_tsne_improved.png', dpi=300, bbox_inches='tight')
#                 plt.close()
#         else:
#             # Si ya tenemos 2 dimensiones, usar directamente
#             plt.figure(figsize=(10, 8))
#             scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], 
#                                  c=cluster_labels, cmap='viridis', s=50, alpha=0.8)
            
#             plt.colorbar(scatter, label='Cluster')
#             plt.title('Visualizaci√≥n de clusters')
#             plt.xlabel('Dimensi√≥n 1')
#             plt.ylabel('Dimensi√≥n 2')
#             plt.tight_layout()
#             plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_direct.png', dpi=300, bbox_inches='tight')
#             plt.close()
        
#         # Visualizar distribuci√≥n de caracter√≠sticas m√°s importantes por cluster
#         # Definir n√∫mero de filas y columnas
        
#         number_of_graphs = len(feature_names)

#         # Calcular din√°micamente las filas y columnas
#         cols = 5  # N√∫mero fijo de columnas
#         rows = int(np.ceil(number_of_graphs / cols))  # Calcula cu√°ntas filas son necesarias

#         # Crear la figura con el n√∫mero adecuado de subgr√°ficos
#         fig, axes = plt.subplots(rows, cols, figsize=(25, rows * 5))  # Altura ajustada din√°micamente
#         axes = axes.flatten()  # Aplanar en una lista 1D

#         # Crear DataFrame con datos originales y etiquetas de cluster
#         data_df = pd.DataFrame(X_clean, columns=feature_names)
#         data_df['cluster'] = cluster_labels

#         # Iterar sobre todas las caracter√≠sticas
#         for i, feature in enumerate(feature_names):
#             sns.boxplot(x='cluster', y=feature, data=data_df, ax=axes[i])
#             axes[i].set_title(f'Distribuci√≥n de {feature}')
#             axes[i].set_xlabel('Cluster')
#             axes[i].set_ylabel(feature)

#         # Ocultar los ejes sobrantes si hay menos gr√°ficos que subplots
#         for j in range(number_of_graphs, len(axes)):
#             fig.delaxes(axes[j])

#         # Ajustar el dise√±o
#         plt.tight_layout()
#         plt.savefig('Resultados/urbano_pattern_cluster/feature_distributions_by_cluster.png', dpi=300, bbox_inches='tight')
#         plt.close()

#     return n_clusters, cluster_labels, centers_df, sorted_features


def optimal_clustering_improved(X, feature_names, n_clusters=None, use_pca=True, 
                               pca_variance_threshold=0.95, max_pca_components=8, 
                               visualize=True, use_elbow_method=False):
    """
    Realiza clustering mejorado con KMeans y an√°lisis de caracter√≠sticas importantes
    
    Par√°metros:
    -----------
    X : array
        Datos de entrada
    feature_names : list
        Nombres de las caracter√≠sticas
    n_clusters : int, opcional
        N√∫mero de clusters. Si es None, se determina autom√°ticamente
    use_pca : bool, por defecto True
        Si se debe usar PCA para reducci√≥n de dimensionalidad
    pca_variance_threshold : float, por defecto 0.95
        Umbral de varianza explicada acumulada para seleccionar componentes
    max_pca_components : int, por defecto 5
        N√∫mero m√°ximo de componentes PCA a usar
    visualize : bool, por defecto True
        Si se deben generar visualizaciones
    use_elbow_method : bool, por defecto False
        Si se debe usar el m√©todo del codo para determinar componentes
    """
    
    # Eliminar filas con NaN o infinitos (tu c√≥digo original)
    valid_rows = ~np.any(np.isnan(X) | np.isinf(X), axis=1)
    X_clean = X[valid_rows]
    if X_clean.shape[0] < X.shape[0]:
        print(f"Eliminadas {X.shape[0] - X_clean.shape[0]} filas con valores no v√°lidos")
    
    # Normalizar caracter√≠sticas
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_clean)
    
    # Reducci√≥n de dimensionalidad
    if use_pca:
        # Determinar n√∫mero √≥ptimo de componentes
        full_pca = PCA().fit(X_scaled)
        cum_var = np.cumsum(full_pca.explained_variance_ratio_)
        
        # Visualizar la varianza explicada acumulada (scree plot)
        if visualize:
            plt.figure(figsize=(10, 6))
            plt.plot(range(1, len(cum_var) + 1), cum_var, marker='o', linestyle='-')
            plt.axhline(y=pca_variance_threshold, color='r', linestyle='--', 
                      label=f'Umbral ({pca_variance_threshold})')
            plt.title('Varianza explicada acumulada vs N√∫mero de componentes')
            plt.xlabel('N√∫mero de componentes')
            plt.ylabel('Varianza acumulada explicada')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig('Resultados/urbano_pattern_cluster/pca_variance_explained.png', dpi=300)
            plt.close()
        
        # M√©todo del codo para PCA si se solicita
        if use_elbow_method:
            # Calcular "aceleraci√≥n" de la curva de varianza
            npc = len(full_pca.explained_variance_ratio_)
            acceleration = np.diff(np.diff(cum_var)) + 0.001  # Evitar dividir por cero
            k_elbow = np.argmax(acceleration) + 1  # El punto donde la aceleraci√≥n es m√°xima
            n_components = min(k_elbow + 1, max_pca_components)  # +1 porque los √≠ndices comienzan en 0
            
            if visualize:
                plt.figure(figsize=(10, 6))
                plt.plot(range(1, npc-1), acceleration, marker='o', linestyle='-')
                plt.axvline(x=k_elbow, color='r', linestyle='--', 
                          label=f'Punto de inflexi√≥n (k={k_elbow+1})')
                plt.title('M√©todo del codo: Aceleraci√≥n de la varianza explicada')
                plt.xlabel('N√∫mero de componentes')
                plt.ylabel('Aceleraci√≥n de varianza')
                plt.grid(True)
                plt.legend()
                plt.tight_layout()
                plt.savefig('Resultados/urbano_pattern_cluster/pca_elbow_method.png', dpi=300)
                plt.close()
        else:
            # Criterio basado en umbral de varianza
            n_components = np.argmax(cum_var >= pca_variance_threshold) + 1
        
        # Aplicar restricciones al n√∫mero de componentes
        n_components = max(2, min(n_components, min(max_pca_components, X_scaled.shape[1])))
        
        print(f"\nSeleccionados {n_components} componentes PCA")
        print(f"Varianza explicada por estos componentes: {cum_var[n_components-1]:.4f}")
        
        # Realizar PCA con el n√∫mero √≥ptimo de componentes
        pca = PCA(n_components=n_components)
        X_reduced = pca.fit_transform(X_scaled)
        
        # An√°lisis de componentes principales
        print(f"\nAn√°lisis PCA con {n_components} componentes:")
        for i, var in enumerate(pca.explained_variance_ratio_):
            print(f"  PC{i+1}: {var:.4f} de varianza explicada")
        print(f"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}")
   
        # Guardar informaci√≥n de los componentes PCA en un archivo de texto
        with open('Resultados/urbano_pattern_cluster/pca_analysis.txt', 'w') as f:
            f.write(f"AN√ÅLISIS DE COMPONENTES PRINCIPALES (PCA)\n")
            f.write(f"======================================\n\n")
            f.write(f"N√∫mero de componentes seleccionados: {n_components}\n")
            f.write(f"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}\n\n")
            
            f.write("VARIANZA EXPLICADA POR COMPONENTE:\n")
            for i, var in enumerate(pca.explained_variance_ratio_):
                f.write(f"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\n")
            f.write("\n")
            
            f.write("CONTRIBUCI√ìN DE VARIABLES A COMPONENTES:\n")
            # Crear un DataFrame con las contribuciones de las caracter√≠sticas a cada componente
            components_df = pd.DataFrame(
                pca.components_.T,
                columns=[f'PC{i+1}' for i in range(n_components)],
                index=feature_names
            )
            
            # Para cada componente, listar las variables m√°s influyentes
            for i in range(n_components):
                pc_name = f'PC{i+1}'
                f.write(f"\n{pc_name} - Explica {pca.explained_variance_ratio_[i]*100:.2f}% de la varianza:\n")
                
                # Ordenar caracter√≠sticas por su contribuci√≥n absoluta a este componente
                component_contrib = components_df[pc_name].abs().sort_values(ascending=False)
                
                # Encontrar variables con mayor contribuci√≥n (positiva y negativa)
                for feature, value in zip(components_df.index, components_df[pc_name]):
                    contribution = abs(value)
                    # Mostrar solo contribuciones significativas (ajustar umbral seg√∫n necesidad)
                    if contribution > 0.2:  # Umbral arbitrario, ajustar seg√∫n sea necesario
                        direction = "positiva" if value > 0 else "negativa"
                        f.write(f"  - {feature}: {value:.4f} (contribuci√≥n {direction})\n")
            
            f.write("\n\nINTERPRETACI√ìN DE COMPONENTES:\n")
            f.write("La interpretaci√≥n de cada componente debe hacerse considerando las variables\n")
            f.write("con mayor contribuci√≥n (positiva o negativa). Variables con contribuciones del\n")
            f.write("mismo signo est√°n correlacionadas positivamente en ese componente, mientras que\n")
            f.write("variables con signos opuestos est√°n correlacionadas negativamente.\n")
        
        # Visualizar contribuci√≥n de caracter√≠sticas a componentes
        if visualize:
            plt.figure(figsize=(12, 8))
            components = pd.DataFrame(
                pca.components_.T,
                columns=[f'PC{i+1}' for i in range(n_components)],
                index=feature_names
            )
            
            sns.heatmap(components, cmap='coolwarm', annot=True, fmt=".2f")
            plt.title('Contribuci√≥n de variables a componentes principales')
            plt.tight_layout()
            plt.savefig('Resultados/urbano_pattern_cluster/pca_components_contributions.png', dpi=300, bbox_inches='tight')
            plt.close()
    else:
        X_reduced = X_scaled
    
    # Encontrar n√∫mero √≥ptimo de clusters si no se proporciona
    if n_clusters is None:
        # Asumimos que la funci√≥n find_optimal_k_improved est√° definida en otro lugar
        n_clusters = find_optimal_k_improved(X_reduced, max_k=8, min_k=3)
    
    print(f"\nRealizando clustering KMeans con {n_clusters} clusters")
    
    # Usar KMeans con inicializaci√≥n k-means++ y m√∫ltiples inicios
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
    cluster_labels = kmeans.fit_predict(X_reduced)
    
    # Analizar centros de clusters
    if use_pca:
        # Proyectar centros al espacio original
        centers_pca = kmeans.cluster_centers_
        centers_original = pca.inverse_transform(centers_pca)
        centers_original = scaler.inverse_transform(centers_original)
    else:
        centers_original = scaler.inverse_transform(kmeans.cluster_centers_)
    
    # Crear DataFrame de centros
    centers_df = pd.DataFrame(centers_original, columns=feature_names)
    centers_df.index = [f'Cluster {i}' for i in range(n_clusters)]
    
    print("\nCaracter√≠sticas de los centros de clusters:")
    print(centers_df)
    
    # Analizar variables m√°s discriminantes entre clusters
    cluster_importance = {}
    for feature in feature_names:
        # Calcular varianza entre clusters para esta caracter√≠stica
        values = centers_df[feature].values
        variance = np.var(values)
        max_diff = np.max(values) - np.min(values)
        importance = variance * max_diff  # Ponderaci√≥n por rango
        cluster_importance[feature] = importance
    
    sorted_features = sorted(cluster_importance.items(), key=lambda x: x[1], reverse=True)
    
    print("\nCaracter√≠sticas m√°s importantes para diferenciar clusters:")
    for feature, importance in sorted_features[:5]:
        print(f"{feature}: {importance:.4f}")
    
    # Visualizar clusters
    if visualize:
        # CORRECCI√ìN: En lugar de usar t-SNE, que puede no preservar distancias globales,
        # usar PCA para visualizaci√≥n si el n√∫mero de caracter√≠sticas es alto
        if X_reduced.shape[1] > 2:
            # Para visualizaci√≥n, usamos PCA directamente desde los datos escalados
            viz_pca = PCA(n_components=2)
            X_viz = viz_pca.fit_transform(X_scaled)
            
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(X_viz[:, 0], X_viz[:, 1], c=cluster_labels, 
                                 cmap='viridis', s=50, alpha=0.8)
            
            # Transformar centros de clusters a 2D para visualizaci√≥n
            if use_pca:
                # Primero a espacio escalado
                centers_scaled = scaler.transform(centers_original)
                # Luego proyectar a 2D con el mismo PCA de visualizaci√≥n
                centers_viz = viz_pca.transform(centers_scaled)
            else:
                centers_viz = viz_pca.transform(kmeans.cluster_centers_)
            
            # Mostrar centros en la visualizaci√≥n
            plt.scatter(centers_viz[:, 0], centers_viz[:, 1], 
                       c='red', s=200, alpha=0.8, marker='X')
            
            # A√±adir etiquetas de clusters
            for i, (x, y) in enumerate(centers_viz):
                plt.annotate(f'Cluster {i}', (x, y), fontsize=12, 
                             ha='center', va='center', color='white',
                             bbox=dict(boxstyle="round,pad=0.3", fc='black', alpha=0.7))
            
            plt.colorbar(scatter, label='Cluster')
            plt.title('Visualizaci√≥n de clusters usando PCA')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.tight_layout()
            plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_pca.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # Adicionalmente, podemos usar t-SNE como visualizaci√≥n complementaria
            # pero con par√°metros m√°s adecuados
            if X_clean.shape[0] > 5:  # Solo si hay suficientes datos
                perplexity = min(30, max(5, X_clean.shape[0] // 10))
                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity,
                           learning_rate='auto', init='pca')
                X_tsne = tsne.fit_transform(X_scaled)
                
                plt.figure(figsize=(10, 8))
                scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], 
                                     c=cluster_labels, cmap='viridis', s=50, alpha=0.8)
                
                plt.colorbar(scatter, label='Cluster')
                plt.title('Visualizaci√≥n de clusters usando t-SNE')
                plt.xlabel('t-SNE 1')
                plt.ylabel('t-SNE 2')
                plt.tight_layout()
                plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_tsne_improved.png', dpi=300, bbox_inches='tight')
                plt.close()
        else:
            # Si ya tenemos 2 dimensiones, usar directamente
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], 
                                 c=cluster_labels, cmap='viridis', s=50, alpha=0.8)
            
            plt.colorbar(scatter, label='Cluster')
            plt.title('Visualizaci√≥n de clusters')
            plt.xlabel('Dimensi√≥n 1')
            plt.ylabel('Dimensi√≥n 2')
            plt.tight_layout()
            plt.savefig('Resultados/urbano_pattern_cluster/cluster_visualization_direct.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # Visualizar distribuci√≥n de caracter√≠sticas m√°s importantes por cluster
        # Definir n√∫mero de filas y columnas
        
        number_of_graphs = len(feature_names)

        # Calcular din√°micamente las filas y columnas
        cols = 5  # N√∫mero fijo de columnas
        rows = int(np.ceil(number_of_graphs / cols))  # Calcula cu√°ntas filas son necesarias

        # Crear la figura con el n√∫mero adecuado de subgr√°ficos
        fig, axes = plt.subplots(rows, cols, figsize=(25, rows * 5))  # Altura ajustada din√°micamente
        axes = axes.flatten()  # Aplanar en una lista 1D

        # Crear DataFrame con datos originales y etiquetas de cluster
        data_df = pd.DataFrame(X_clean, columns=feature_names)
        data_df['cluster'] = cluster_labels

        # Iterar sobre todas las caracter√≠sticas
        for i, feature in enumerate(feature_names):
            sns.boxplot(x='cluster', y=feature, data=data_df, ax=axes[i])
            axes[i].set_title(f'Distribuci√≥n de {feature}')
            axes[i].set_xlabel('Cluster')
            axes[i].set_ylabel(feature)

        # Ocultar los ejes sobrantes si hay menos gr√°ficos que subplots
        for j in range(number_of_graphs, len(axes)):
            fig.delaxes(axes[j])

        # Ajustar el dise√±o
        plt.tight_layout()
        plt.savefig('Resultados/urbano_pattern_cluster/feature_distributions_by_cluster.png', dpi=300, bbox_inches='tight')
        plt.close()

    return n_clusters, cluster_labels, centers_df, sorted_features


def urban_pattern_clustering(
    stats_dict, 
    graph_dict,
    classify_func, 
    geojson_file,
    n_clusters=None,
    output_dir="Resultados/urbano_pattern_cluster"
    ):
    """
    Versi√≥n mejorada para clustering de patrones urbanos
    """
    
    # Crear directorio para resultados
    os.makedirs(output_dir, exist_ok=True)
    
    # Cargar GeoDataFrame
    gdf = gpd.read_file(geojson_file)
    print(f"GeoDataFrame cargado con {len(gdf)} pol√≠gonos")
    
    # Preparar caracter√≠sticas mejoradas para clustering
    X, poly_ids, feature_names = prepare_clustering_features_improved(stats_dict, graph_dict)
    print(f"Caracter√≠sticas preparadas para {len(X)} pol√≠gonos con {len(feature_names)} variables")
    
    # Realizar clustering mejorado
    n_clusters, cluster_labels, centers_df, important_features = optimal_clustering_improved(
        X, feature_names, n_clusters=n_clusters
    )
    
    # Clasificaci√≥n original de patrones
    original_patterns = []
    valid_poly_ids = []
    
    for poly_id in poly_ids:
        poly_stats = stats_dict[poly_id]
        category = classify_func(poly_stats)
        original_patterns.append(category)
        valid_poly_ids.append(poly_id)
    
    # Crear DataFrame con resultados
    results_df = pd.DataFrame({
        'poly_id': [pid[0] for pid in valid_poly_ids],
        'subpoly_id': [pid[1] for pid in valid_poly_ids],
        'original_pattern': original_patterns,
        'cluster': cluster_labels
    })
    
    # An√°lisis de relaci√≥n entre patrones originales y clusters
    pattern_cluster_matrix = pd.crosstab(
        results_df['original_pattern'], 
        results_df['cluster'],
        normalize='columns'
    ) * 100
    
    print("\nDistribuci√≥n de patrones por cluster (%):")
    print(pattern_cluster_matrix)
    
    # Visualizar matriz de patrones vs clusters como heatmap
    plt.figure(figsize=(10, 6))
    sns.heatmap(pattern_cluster_matrix, annot=True, fmt=".1f", cmap="YlGnBu")
    plt.title('Distribuci√≥n de patrones urbanos por cluster (%)')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'pattern_cluster_heatmap.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # Determinar el patr√≥n dominante para cada cluster
    cluster_dominant_pattern = {}
    for cluster in range(n_clusters):
        if cluster in pattern_cluster_matrix.columns:
            dominant_pattern = pattern_cluster_matrix[cluster].idxmax()
            percentage = pattern_cluster_matrix.loc[dominant_pattern, cluster]
            cluster_dominant_pattern[cluster] = (dominant_pattern, percentage)
    
    # Asignar nombres distintivos a los clusters
    cluster_names = {}
    named_patterns = set()
    
    for cluster, (pattern, percentage) in cluster_dominant_pattern.items():
        if pattern in named_patterns:
            # Si ya existe un cluster con este patr√≥n, agregar un sufijo
            similar_clusters = [c for c, (p, _) in cluster_dominant_pattern.items() if p == pattern]
            suffix = len([c for c in similar_clusters if c < cluster]) + 1
            
            # Analizar diferencias con otros clusters del mismo patr√≥n
            distinctive_features = []
            for feature, _ in important_features[:3]:
                feature_value = centers_df.loc[f'Cluster {cluster}', feature]
                avg_value = centers_df.loc[[f'Cluster {c}' for c in similar_clusters if c != cluster], feature].mean()
                if feature_value > avg_value * 1.2:
                    distinctive_features.append(f"alto_{feature.split('_')[0]}")
                elif feature_value < avg_value * 0.8:
                    distinctive_features.append(f"bajo_{feature.split('_')[0]}")
            
            if distinctive_features:
                cluster_names[cluster] = f"{pattern}_{distinctive_features[0]}"
            else:
                cluster_names[cluster] = f"{pattern}_{suffix}"
        else:
            cluster_names[cluster] = pattern
            named_patterns.add(pattern)
    
    # Filtrar y preparar GeoDataFrame para visualizaci√≥n
    poly_id_map = {pid[0]: i for i, pid in enumerate(valid_poly_ids)}
    valid_indices = [i for i in poly_id_map.keys() if i < len(gdf)]
    gdf_filtered = gdf.loc[valid_indices].copy()
    
    # A√±adir resultados al GeoDataFrame
    gdf_filtered['original_pattern'] = gdf_filtered.index.map(
        lambda idx: results_df[results_df['poly_id'] == idx]['original_pattern'].values[0] 
        if idx in poly_id_map and any(results_df['poly_id'] == idx) else 'unknown'
    )
    
    gdf_filtered['cluster'] = gdf_filtered.index.map(
        lambda idx: results_df[results_df['poly_id'] == idx]['cluster'].values[0] 
        if idx in poly_id_map and any(results_df['poly_id'] == idx) else -1
    )
    
    gdf_filtered['cluster_name'] = gdf_filtered['cluster'].map(
        lambda c: cluster_names.get(c, f"cluster_{c}") if c != -1 else 'unknown'
    )
    
    # Definir esquema de colores mejorado para patrones urbanos
    # Usando colores distintivos y tem√°ticos para cada tipo
    color_map = {
        'cul_de_sac': '#FF6B6B',   # Rojo para callejones sin salida
        'gridiron': '#006400',     # Verde oscuro para grid
        'organico': '#45B7D1',     # Azul para org√°nico
        'hibrido': '#FDCB6E',      # Amarillo para h√≠brido
        'unknown': '#CCCCCC'       # Gris para desconocidos
    }
    
    # A√±adir colores para nombres de cluster
    for cluster, name in cluster_names.items():
        pattern = name.split('_')[0]
        if pattern in color_map:
            base_color = np.array(mcolors.to_rgb(color_map[pattern]))
            # Ajustar color ligeramente para distinguir patrones similares
            if '_' in name and pattern in named_patterns:
                # Oscurecer o aclarar el color seg√∫n el sufijo
                suffix = name.split('_')[1]
                if suffix.startswith('alto'):
                    # M√°s claro
                    adjusted_color = base_color + (1 - base_color) * 0.3
                elif suffix.startswith('bajo'):
                    # M√°s oscuro
                    adjusted_color = base_color * 0.7
                else:
                    # Alternar entre tonos
                    factor = int(suffix) * 0.15 if suffix.isdigit() else 0.2
                    adjusted_color = base_color + np.array([0, factor, -factor])
                
                # Recortar valores a rango v√°lido
                adjusted_color = np.clip(adjusted_color, 0, 1)
                color_map[name] = mcolors.to_hex(adjusted_color)
            else:
                color_map[name] = color_map[pattern]
    
    # Visualizaci√≥n de comparaci√≥n
    fig, axes = plt.subplots(1, 2, figsize=(18, 10))
    
    # Funci√≥n para asignar colores
    def get_color(value, color_map):
        return color_map.get(value, '#CCCCCC')
    
    # Mapa de patrones originales
    gdf_filtered['color_pattern'] = gdf_filtered['original_pattern'].apply(
        lambda x: get_color(x, color_map)
    )
    
    gdf_filtered.plot(color=gdf_filtered['color_pattern'], 
                     ax=axes[0],
                     edgecolor='black', 
                     linewidth=0.5)
    
    # Crear leyenda para patrones
    pattern_legend_elements = [
        Patch(facecolor=color_map[pattern], edgecolor='black', label=pattern)
        for pattern in sorted(list(set(gdf_filtered['original_pattern'])))
        if pattern in color_map
    ]
    axes[0].legend(handles=pattern_legend_elements, loc='lower right', title="Patrones originales")
    axes[0].set_title('Patrones de calle te√≥ricos', fontsize=14)
    axes[0].axis('off')
    
    # Mapa de clusters
    gdf_filtered['color_cluster'] = gdf_filtered['cluster_name'].apply(
        lambda x: get_color(x, color_map)
    )
    
    gdf_filtered.plot(color=gdf_filtered['color_cluster'], 
                     ax=axes[1],
                     edgecolor='black', 
                     linewidth=0.5)
    
    # Crear leyenda para clusters
    cluster_legend_elements = [
        Patch(facecolor=get_color(name, color_map), edgecolor='black', label=name)
        for name in sorted(list(set(gdf_filtered['cluster_name'])))
        if name != 'unknown'
    ]
    axes[1].legend(handles=cluster_legend_elements, loc='lower right', title="Clusters identificados")
    axes[1].set_title('Agrupaci√≥n por caracter√≠sticas morfol√≥gicas', fontsize=14)
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'urban_pattern_comparison.png'), 
                dpi=300, 
                bbox_inches='tight')
    plt.close()
    
    # An√°lisis de caracter√≠sticas por tipo de patr√≥n urbano
    pattern_feature_summary = {}
    
    # A√±adir valores normalizados de caracter√≠sticas al DataFrame
    feature_data = pd.DataFrame(X, columns=feature_names)
    feature_data['pattern'] = original_patterns
    feature_data['cluster'] = cluster_labels
    
    # Calcular estad√≠sticas por patr√≥n
    for pattern in set(original_patterns):
        pattern_data = feature_data[feature_data['pattern'] == pattern]
        pattern_feature_summary[pattern] = {
            feature: pattern_data[feature].mean() for feature in feature_names
        }
    
    # Crear DataFrame de resumen
    summary_df = pd.DataFrame(pattern_feature_summary).T
    
    # Guardar resultados en formato Excel
    with pd.ExcelWriter(os.path.join(output_dir, 'urban_pattern_analysis.xlsx')) as writer:
        # Hoja 1: Resumen de patrones y caracter√≠sticas
        summary_df.to_excel(writer, sheet_name='Pattern_Features')
        
        # Hoja 2: Matriz de confusi√≥n entre patrones y clusters
        pattern_cluster_matrix.to_excel(writer, sheet_name='Pattern_Cluster_Matrix')
        
        # Hoja 3: Caracter√≠sticas de centros de clusters
        centers_df.to_excel(writer, sheet_name='Cluster_Centers')
        
        # Hoja 4: Importancia de caracter√≠sticas
        pd.DataFrame(important_features, columns=['Feature', 'Importance']).to_excel(
            writer, sheet_name='Feature_Importance'
        )
    
    # Guardar GeoJSON con resultados
    gdf_filtered.to_file(
        os.path.join(output_dir, 'urban_patterns_clustered.geojson'),
        driver='GeoJSON'
    )
    
    return {
        'geodataframe': gdf_filtered,
        'cluster_centers': centers_df,
        'pattern_cluster_matrix': pattern_cluster_matrix,
        'cluster_names': cluster_names,
        'important_features': important_features,
        'n_clusters': n_clusters
    }




# 1. Cargar el GeoJSON
print("Cargando archivo GeoJSON...")
geojson_file = "Poligonos_Medellin/Json_files/EOD_2017_SIT_only_AMVA_URBANO.geojson"
gdf = gpd.read_file(geojson_file)
print(f"GeoDataFrame cargado con {len(gdf)} pol√≠gonos")
stats_txt = "Poligonos_Medellin/Resultados/poligonos_stats_ordenado.txt"
graph_dict = procesar_poligonos_y_generar_grafos(gdf)


stats_dict = load_polygon_stats_from_txt(stats_txt)
resultados = urban_pattern_clustering(
    stats_dict, 
    graph_dict,
    classify_polygon, 
    geojson_file,
    n_clusters= None # Autom√°ticamente determinar√° el n√∫mero √≥ptimo
)